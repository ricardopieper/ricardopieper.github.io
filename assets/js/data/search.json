[ { "title": "The pitfalls of Redis", "url": "/posts/redis-pitfalls/", "categories": "Redis", "tags": "redis, concurrent, distributed", "date": "2023-11-07 12:00:00 -0300", "snippet": "Over many years, I‚Äôve been using Redis for caching and as a ‚Äúdistributed data structure server‚Äù. I love it. As an extremely lightweight in-memory database that Just Works‚Ñ¢Ô∏è, I‚Äôve seen developers slap it onto a backend system as a caching system to make things go fast, reduce database load, or do distributed programming in some capacity.I really like Redis beyond the usual caching scenarios. If your system is responsible for a funcionality that needs temporary data to operate in a distributed manner without persistence, it‚Äôs great. You don‚Äôt need a SQL database that could potentially be slow depending on what you‚Äôre doing, or scheduling tasks onto a job queue to clean data after you‚Äôre done. Redis has all of that, and also serves as a backend for job systems such as Quartz in Java or BullMQ in Node. Redis‚Äôs documentation is also pretty good. All the commands are right in this page and it‚Äôs easy to locate the command you need.Redis is amazing. But in this article I want to explore a bug I had where you need to be careful about how you store data in Redis and how you interact with it. This article requires some basic understanding of Redis to read. I will explore one of the bugs I saw in my carrer, but I had others that worked in a similar manner in other systems.Too much data in one keyWhen I started working at Monomyto Game Studios, one of the most annoying sporadic bugs was related to squad management. Gunstars is a battle royale top-down shooter game where you can join games in a squad or alone. Sometimes, you would invite someone to a squad, the other person would accept, and only one of them would end up in the squad, while the other person would, IIRC, be left in an invalid state. They would join the squad, their session data would say as much, but the client wouldn‚Äôt show them there.In the backend, we had multiple servers, and each client could be connected in any of these servers. Inside the backend, the clients would communicate among themselves by sending messages. This was done using a framework called MagicOnion, but we don‚Äôt need to go in depth in what MagicOnion does. Suffice to say, it made this communication easier. During the process of building a squad, both clients would add themselves to the same squad at the same time in Redis. The astute reader that does distributed programming on a daily basis (or even just concurrent programming on a shared memory architecture) can spot a potential problem right there.The squad was represented in Redis as a single key, with a name similar to squad_82506af6-2046-4688-a472-b31569ff974e and contents like this:{ &quot;SquadLeader&quot;: &quot;155afdaa-6b2d-4478-b10a-05094375f1bf&quot; //the User ID of the squad leader, &quot;GameMode&quot;: &quot;DuoBattleRoyale&quot;, //...some other data, &quot;Members&quot;: [ { &quot;UserId&quot;: &quot;155afdaa-6b2d-4478-b10a-05094375f1bf&quot;, //...other data }, { &quot;UserId&quot;: &quot;82506af6-2046-4688-a472-b31569ff974e&quot;, //... } ]}In the ‚Äúother data‚Äù, I think we copied some user profile information, like the current level of the character, their weapons, equipments, and so on. This was unecessary and some of that was removed. But it also would store some matchmaking information.But other than that‚Ä¶ this representation for a squad seems reasonable? ü§® What‚Äôs the problem?Concurrency in RedisRedis is very fast, but runs single threaded. That fact seems to give developers free reign to just send commands to it without thinking about concurrent modifications to a key. Redis will just work, right?However, the ‚Äúsingle threaded‚Äù, ‚Äúsequential‚Äù part only really applies to modifying the state of the keys. You can‚Äôt predict the order in which commands are sent in a distributed scenario without adding some sort of synchronization, which can be a huge can of worms. You can‚Äôt predict exactly how fast the different concurrent tasks in your system will run, they might get preempted by the OS, they might just run a bit slower due to increased system load in one server while the other servers are running with lower load, and so on.In Gunstars servers, the 2 connections, potentially in separate servers, would communicate among them about the squad invitation, and when the invitee accepted the invite, they would send a notification to the leader, and then add themselves to the squad. The leader, upon receiving that notification, would also add themselves to the squad. So we have a situation where both are adding themselves to the squad, in parallel.This is how the invitee would behave: Send the InviteAccepted event asynchronously, fire and forget Without waiting for the leader to add themselves, we get the current squad data: GET squad_82506af6-2046-4688-a472-b31569ff974e Deserialize, add themselves to the Members section (done in C#) Serialize Set the new squad data with SET squad_82506af6-2046-4688-a472-b31569ff974e: { &quot;SquadLeader&quot;: &quot;155afdaa-6b2d-4478-b10a-05094375f1bf&quot;, &quot;GameMode&quot;: &quot;DuoBattleRoyale&quot;, &quot;Members&quot;: [ { &quot;UserId&quot;: &quot;155afdaa-6b2d-4478-b10a-05094375f1bf&quot;, //... } ] } So far so good. How about the leader? Receive the InviteAccepted event GET squad_82506af6-2046-4688-a472-b31569ff974e Deserialize, add themselves to the Members section (done in C#) Set the new squad data with SET squad_82506af6-2046-4688-a472-b31569ff974e: { &quot;SquadLeader&quot;: &quot;155afdaa-6b2d-4478-b10a-05094375f1bf&quot;, &quot;GameMode&quot;: &quot;DuoBattleRoyale&quot;, &quot;Members&quot;: [ { &quot;UserId&quot;: &quot;82506af6-2046-4688-a472-b31569ff974e&quot;, //... }, { &quot;UserId&quot;: &quot;155afdaa-6b2d-4478-b10a-05094375f1bf&quot;, //... } ] } Notice that in this example, it just worked. However, we got very lucky: the leader ran GET squad_82506af6-2046-4688-a472-b31569ff974e after the invitee added themselves. What if the invitee takes a bit longer to run, making both run GET squad_82506af6-2046-4688-a472-b31569ff974e exactly at the same time?In that case, both would run GET squad_82506af6-2046-4688-a472-b31569ff974e and see the same state:{ &quot;SquadLeader&quot;: &quot;155afdaa-6b2d-4478-b10a-05094375f1bf&quot;, &quot;GameMode&quot;: &quot;DuoBattleRoyale&quot;, &quot;Members&quot;: []}Both would add themselves to that json:{ &quot;SquadLeader&quot;: &quot;155afdaa-6b2d-4478-b10a-05094375f1bf&quot;, &quot;GameMode&quot;: &quot;DuoBattleRoyale&quot;, &quot;Members&quot;: [ { &quot;UserId&quot;: &quot;&amp;lt;either the leader or invitee&amp;gt;&quot;, } ]}And then both would store that key to Redis. Who will win that race?{ &quot;SquadLeader&quot;: &quot;155afdaa-6b2d-4478-b10a-05094375f1bf&quot;, &quot;GameMode&quot;: &quot;DuoBattleRoyale&quot;, &quot;Members&quot;: [ { &quot;UserId&quot;: &quot;&amp;lt;Maybe the leader won? Maybe the invitee? Who knows!&amp;gt;&quot;, } ]}If you didn‚Äôt understand the definition of a race condition, I hope that gives you an example beyond the basic multithreaded x += 1 that is common to find in the internet.Turns out in this case we are storing way too much stuff in one key. We can rework this and separate the members from the general squad data.The solutionIn a shared memory architecture, one could simply add a Mutex (or lock) on the squad data and be done with it. This makes all concurrent threads synchronize on the modifications to that list.In a distributed memory architecture, we don‚Äôt have that. Instead, what we need is a distributed lock. It just makes sense.I am grug brained dev. I see multi thread race condition, I use lock. I see distributed race condition, I use distributed lock.That would make it just work. There‚Äôs even an algorithm called Redlock that is implemented for many languages, including .NET. Let‚Äôs just slap that shit on our codebase and we‚Äôll be done with it, right?No. Please don‚Äôt. That‚Äôs not how grugbrain.dev works. grug brain do not like complexity. distributed lock very complex. me no like complex, me like simple. Let‚Äôs think about what redis actually is: It‚Äôs a data structure server. Data structures‚Ä¶ sets, lists, hashes‚Ä¶Lists. We can use a Redis List to solve that problem. Let‚Äôs think about Redis being single threaded again: it means that operations on the Redis server state are applied sequentially. You can think of Redis‚Äôs List commands like a buffed up version of ConcurrentBag in C# that also works in a distributed setting.Let‚Äôs get rid of the ‚ÄúMembers‚Äù key in that JSON: { &quot;SquadLeader&quot;: &quot;155afdaa-6b2d-4478-b10a-05094375f1bf&quot;, &quot;GameMode&quot;: &quot;DuoBattleRoyale&quot;, &quot;MatchmakingStatus&quot;: &quot;Waiting&quot;, //...}Now, when the InviteAccepted event is sent or received, each member can then simply add themselves to a new squadmembers list. The invitee would simply send this command:LPUSH squadmembers_82506af6-2046-4688-a472-b31569ff974e 82506af6-2046-4688-a472-b31569ff974eAnd the leader would send this command:LPUSH squadmembers_82506af6-2046-4688-a472-b31569ff974e 155afdaa-6b2d-4478-b10a-05094375f1bfThe commands are interpreted by Redis sequentially, the LPUSH behaves correctly and does not overwrite the previous data. Now both members are in the squad.AtomicityRather than ‚Äútoo much data in one key‚Äù, the main mistake here is thinking that, because Redis is single-threaded, operations will be magically atomic. But you have to remember: If a key can be modified by many parties at exactly the same time, you have to use atomic operations. However, I think that ‚Äútoo much data in one key‚Äù also means ‚Äútoo much responsibility‚Äù in one key, which is ripe for bugs.That means you shouldn‚Äôt read the whole data from Redis, modify it in the application code, and then write back.[App] var newCounter = redis.Get&amp;lt;int&amp;gt;(&quot;COUNTER&quot;) + 1[Redis] GET counter (Suppose it returns 0)[App] redis.Set(&quot;COUNTER&quot;, newCounter)[Redis] SET counter 1This works if there‚Äôs only a single user modifying this counter, but suppose there‚Äôs more:[User1] var newCounter = redis.Get&amp;lt;int&amp;gt;(&quot;COUNTER&quot;) + 1[Redis] GET counter (Suppose it returns 0)[User2] var newCounter = redis.Get&amp;lt;int&amp;gt;(&quot;COUNTER&quot;) + 1[Redis] GET counter (returns 0)[User1] redis.Set(&quot;COUNTER&quot;, newCounter)[Redis] SET counter 1[User2] redis.Set(&quot;COUNTER&quot;, newCounter)[Redis] SET counter 1This makes the counter 1 instead of what we really want, which is 2. The solution is to use an atomic operation, like INCR:[User1] var newCounter = redis.Incr(&quot;COUNTER&quot;)[Redis] INCR COUNTER (returns 1)[User2] var newCounter = redis.Incr(&quot;COUNTER&quot;)[Redis] INCR COUNTER (returns 2)You can‚Äôt predict what a specific user‚Äôs newCounter will be, but they will increase monotonically.Network trafficStoring too much data in one key is also bad for performance. Redis can handle many thousands of operations per second, maybe millions. But what happens when you store too much stuff in one key?One possible problem is too much network I/O, to the point of saturating the network capabilities of the server. When I worked on a chat system, we had a module with a ‚Äúmain loop‚Äù responsible for handing out chats to customer service agents, using an algorithm that relied on Redis heavily. Even when we had only a couple thousand users, I saw that system doing 150MB/s and still somehow working! I didn‚Äôt work on that system directly but we all could see the amount of data in the monitoring systems.Unfortunately, it‚Äôs surprisingly hard for some newer developers to understand how much data there is in just 1MB of pure text. And no, that Redis instance was not streaming video, audio or transferring images. It was purely chat and some control messages for the handout of conversations for agents. Some estimates I found on the internet tell me 1MB is like a book with 500 pages. That‚Äôs a lot! Imagine 150 of these books per second.That system would not work for much longer if action wasn‚Äôt be taken. It was also not a trivial problem to solve. A few months after we first saw it, COVID happened and rapidly the amount of users grew, so eventually it had to be solved or mitigated. I don‚Äôt know what the solution was, but I‚Äôm fairly confident we had many keys such as these:{ &quot;status&quot;: &quot;...&quot;, &quot;name&quot;: &quot;...&quot;, &quot;email&quot;: &quot;...&quot;, &quot;channel&quot;: &quot;whatsapp&quot;, //... many properties... &quot;lastMessage&quot;: { //... a fairly big JSON object here }}This key would be fetched in its entirety even when only a status was necessary. Therefore many KBs worth a data would be fetched instead of just a few bytes, increasing the I/O to staggering levels for that amount of users. One way to quickly mitigate this problem is to create a read-only Redis replica and do reads from that server, this way the chat control system doesn‚Äôt affect the other parts of the whole system. IIRC, eventually the keys had some redesign to remove rarely used properties from the main keys.ConclusionHopefully I demonstrated the concept of data races and atomicity in Redis in a way you can understand. Usually these concepts are only explained with shared memory architectures, with mutexes and so on. Turns out Redis does no magic and you will shoot yourself in the foot if you don‚Äôt think about these concepts. I also hopefully explained the network traffic problem well enough so that you really think about what you‚Äôre storing in Redis and how much of that you need on the hot paths of your system.Here‚Äôs a line adapted from Murphy‚Äôs Law: If two users can modify a key at the exact same time, they eventually will, and the ensuing explosion will break your system in the worst way possible. That was a bit dramatic." }, { "title": "How I improved my interpreter speed by 27x - A Compiler/Interpreter Competition", "url": "/posts/rinha-compiler/", "categories": "Rust, Interpreters", "tags": "rust, interpreters, tree walker, bytecode, virtual machine", "date": "2023-10-17 12:00:00 -0300", "snippet": "In September 2023, one of the most anticipated events in Brazil took place: the ‚ÄúRinha de Compiladores‚Äù. This is translated roughly to ‚ÄúCompiler Battle‚Äù, ‚ÄúCompiler Bout‚Äù, or ‚ÄúCompiler Fight‚Äù. If you don‚Äôt speak Portuguese, ‚Äúrinha‚Äù is pronounced like ‚Äúrinya‚Äù and not ‚Äúrin haa‚Äù, the nh is pronounced like √± in Spanish.This is part of an ongoing series of Rinhas. The previous one was a backend-focused competition to see which implementation of a simple HTTP API was the fastest.As of the time of writing this text, the Rinha Frontend is taking place, where the goal is to render a huge JSON with many thousands of elements in a tree-viewer-like structure in the least amount of time. The compiler rinha was actually more of an interpreter one, as there were very few compilers (or transpilers).The masterminds of the competition are Gabrielle and Sofia. Gabrielle is a 17-year-old girl and Sofia is 21. When I learned about this, I could only remind myself of the time I was 17 years old, playing Counter-Strike all day, playing RF Online, and maybe allegedly probably eating spoonfuls of dirt. This was the general feeling of the Rinha Discord server.Therefore, although I have some criticism about how the benchmarks were made, what they pulled off is nothing short of incredible. Hundreds of people engaged in the competition. Some of them had never written an interpreter. Now they see this is not some sort of crazy magical code. It‚Äôs just regular everyday code, as Jonathan Blow says.Some implementations were participating only for the memes, and couldn‚Äôt care less about performance. They were designed to ‚Äúprovide the highest joy to the developer‚Äù, as did this person. This interpreter was made in Bash and definitely deserves the Most Based Implementation prize, but unfortunately, there was no such category.I ended up not winning the competition. This 27x perf improvement mentioned in the title is compared to my initial implementation, and that 27x only happened after the Rinha finished. My submission only had an 18x improvement over the initial implementation. I admit it‚Äôs a bit of a clickbait title, but all will be clear in the end. You can check the results by clicking here.I ended up in 18th out of almost 200 submissions (only about 70 working implementations though lol). However, the benchmarking process had some hiccups and was done in a bit of a rush as the organizers admitted. It could very well be that, if the benchmarks were done differently, I could end up even lower in the ranks, but I hope I would finish in a better position.Some really good implementations, including some of the fastest ones, did not end up in the top 3. I remember the fastest interpreter made in C following the excellent Crafting Interpreters book finished in 7th place. Some even used LLVM and could not hit the top 5. However, I am happy that the winner Raphael Victal actually had a very good implementation, though I‚Äôm biased because we had some similar ideas. My interpreter ended up with a score of around 30% less than him, and my interpreter was in fact around 30% slower than his.While I am a bit disappointed with the results, the reality is I shouldn‚Äôt even be on that board, as my submission wouldn‚Äôt even build 3 minutes before the scores were revealed. Sofia was gracious enough to quickly run my fixed build moments before the reveal livestream, but I think I should‚Äôve been disqualified.An interpreter usually has 3 phases, as I‚Äôm gonna explain in the next paragraphs. Bear in mind that for the Rinha language, we didn‚Äôt need to write a parser, as the organizers provided a Rust program that would parse the language and spit out the JSON AST. If you don‚Äôt know what those words mean, don‚Äôt worry, the next paragraphs explain it.Interpreter phasesTo run code, we don‚Äôt simply get input text and start running/compiling it. Maybe in the past, some languages actually did it because resources were so scarce, but nowadays this is not the case. Interpreters and compilers usually have multiple steps and each one gives more and more structure to the input program.LexerThe lexer is the simplest part of the interpreter: it takes input strings and returns a stream/list of tokens.In the Rinha language, function declarations look like this (I‚Äôll be using JS syntax highlighting for Rinha examples):let add = fn(x) =&amp;gt; { x + 1 };This is, of course, a function that just returns its parameter. A tokenizer could read this and return a list of the following tokens:[ Token::LetKeyword, Token::Identifier(&quot;add&quot;), Token::Operator(Operator::Assign), Token::FunctionDeclarationKeyword, Token::OpenParen, Token::Identifier(&quot;x&quot;), Token::ClosedParen, Token::OpenCurlyBraces, Token::Identifier(&quot;x&quot;), Token::Operator(Operator::Plus), Token::IntLiteral(1), Token::CloseCurlyBraces]In this case, the only string we have is in identifiers. We could have more strings if I used a string literal somewhere. Identifiers are used for variable names, function names, parameter names, type names, and everything that identifies something in the program. As for the other tokens, I think they are self-explanatory.That‚Äôs all a lexer really does. We are now ready for parsing.ParserIn 2023, only a madman would be satisfied with just the tokens, and then just interpret them as is. We need more structure. For that, we build an AST, an abstract syntax tree. This process is so mechanical and boring that there are programs you can use to generate parsers for a language. You describe the grammar, and the language makes a parser for you. I‚Äôm not a big fan, but I do see why people use them. Some of these tools are ANTLR, Bison, Flex, LALRPOP, and others. You can also write your own Recursive Descent parser, which is described in the Crafting Interpreters book. I really recommend reading this book as it will give you the recipe to build an interpreter‚Ä¶. but I also have to say: It‚Äôs a lot of fun if you write one in your own way. Then consult the literature to see which mistakes you made. You will never forget how dumb your ideas actually are, and you won‚Äôt forget about the better idea in the book or even tricks you didn‚Äôt realize could be done.Anyway, at the end of parsing, you‚Äôll end up with a structure similar to this:LetExpression { name: &quot;add&quot;, expression: FunctionDeclaration { parameters: [&quot;x&quot;], body: { BinaryOp( op: Operator::Plus, lhs: Variable(&quot;x&quot;), rhs: IntLiteral(1) ) } }}The astute reader will understand how each piece of this representation relates back to the source code. For Rinha, we also had location information for everything, so what we knew where exactly in the source code each piece was declared.Post-parsingAfter you parse the input text, you have so many paths you can follow I can‚Äôt describe every single one in length, so here‚Äôs a short description. If you know what tree-walking, bytecode VM, JIT, or AOT mean, you can skip this part. Tree-walking interpreter: Just recursively walk through the AST and execute the nodes. Along the way, you have to store the currently declared variables and functions. For Rinha, when you find a function, you can evaluate the function as a Function value, just like any other kind of value. For functions, you can store the entire AST data in this value, the parameter names, and a copy of the current environment (like declared variables) that will act as the function closure. This is a bit slow and you can definitely optimize things out, as I‚Äôll show in the rest of the article. Bytecode VM: Do you know Java? Do you know Javascript? Maybe Python? All of those contain a Virtual Machine (VM) that executes what‚Äôs called a bytecode, which is a list of instructions similar to Assembly, but for a virtual machine (and not a physical machine like your CPU). The bytecode is designed in tandem with the virtual machine so that the VM can execute the bytecode efficiently. You can think of your physical processor as a physical machine that interprets some ‚Äú‚Äúbytecode‚Äù‚Äù (machine instructions) and all of that happens in silicon, in hardware. The bytecode VM tries to emulate some of the things the CPU and OS do, like decoding instructions, doing arithmetic, branching (if statements), call stacks, and so on. This tends to be a lot faster than tree-walking interpreters because there‚Äôs less pointer chasing and better data locality: the AST is a tree structure often with many things allocated all over the heap. Bytecode on the other hand is a more compact representation that fits much better in CPU cache. One of the main overheads of interpreting code is determining ‚Äúwhat to do next?‚Äù, and having a compact representation of the code in the CPU cache helps a lot. It also turns out my approach of HOAS, or what I call Lambda compilation (because it‚Äôs all boxed lambda functions) also helps with that. JIT (Just In Time) compilers: In comparison with native code, bytecode still leaves a lot of performance on the table. A simple instruction in an interpreted language takes many dozens of machine code instructions on a CPU. These instructions would take care of fetching data, moving the instruction pointer, managing the call stack, and so on. These things are emulated in software, which is going to be slower than running machine code on a physical CPU. But interpreted languages do come with a lot of flexibility and portability. What JIT compilers do is to get the best of the two worlds: at its core, the language is interpreted, but parts of the user‚Äôs code are compiled down to fast machine code. Sometimes the compiler can figure all of the types of a function even in languages without type annotations (like JS). This is why Javascript is so fast: what V8, JSC, and Spidermonkey do is nothing short of a miracle, and that‚Äôs due to JIT. When needed and possible, the JIT can generate native code for a function. The same compiled binary or JS file can run in any browser on any computer while retaining really good, sometimes near-native performance. Java and C# are languages that run in a JIT compiler (JVM and RyuJIT respectively, and more recently Java also runs in GraalVM). AOT (Ahead Of Time) compilers: This is what Rust, C++, C, Go, and many other languages do. They compile to native code from the beginning. No bytecode VM needed, and you get native performance as a result. You lose the ability to run the same binary in multiple OSs and CPUs, but you gain a lot of performance. Each OS and CPU architecture needs a different binary.The Rinha LanguageLet‚Äôs take a look at the Rinha language for a bit to understand how to run it.let fib = fn (n) =&amp;gt; { if (n &amp;lt; 2) { n } else { fib(n - 1) + fib(n - 2) }};print(&quot;fib: &quot; + fib(10))Rinha is a small dynamically typed functional language. Functions support recursion and you can declare functions inside functions, return functions from functions, take functions by a parameter that, when executed, returns new functions, and all that good stuff from functional programming.I also wrote a small program that tests some of the language features, except for strings:let iter = fn (from, to, call, prev) =&amp;gt; { if (from &amp;lt; to) { let res = call(from); iter(from + 1, to, call, res) } else { prev }};let work = fn(x) =&amp;gt; { let work_closure = fn(y) =&amp;gt; { let xy = x * y; let tuple = (xy, x); let f = first(tuple); let s = second(tuple); f * s }; iter(0, 200, work_closure, 0)};let iteration = iter(0, 100, work, 0);print(iteration)This program does not do anything useful, but it exercises a few things: Tuples (always 2 elements, fetch them using built-in functions first and second) Closures High-order functions (take functions by parameter and execute them) Tail-calls and Tail-call optimization Some binary operatorsThis was my benchmarking script for the rest of the competition. Turns out the final tests didn‚Äôt go that route, but I wanted this thing to run faster and faster.Just for curiosity, you can build lists in Rinha by using tuples: since tuples can have tuples inside them, you could have something like (1, (2, (3, (4, &quot;&amp;lt;nil&amp;gt;&quot;)))) or something like that. You can also have maps using a clever closure trick:let empty_map = fn(v) =&amp;gt; { (&quot;&amp;lt;KeyNotFound&amp;gt;&quot;, &quot;undefined key &quot; + v)};let set = fn(var, val, m) =&amp;gt; { fn(v) =&amp;gt; { if (var == v) { val } else { m(v) } }};let map = set(&quot;z&quot;, 1, empty_map);let map = set(&quot;x&quot;, 2, map);let v = map(&quot;z&quot;);print(v)The inner function fn(v) inside set takes the current map (represented by the function m) by closure. In fact, all set parameters are captured by the inner function when set is executed, so you can use closures as a mechanism to hold state. To fetch a value, we just recursively call m until we find a closure whose var is our target. It‚Äôs a slow O(n) map, for sure, but a map nonetheless.It‚Äôs interesting to see how powerful functional programming can be: this language does not have built-in types for lists or maps, and yet we can represent those concepts, albeit not in the most performant way. Nonetheless, this idea was used by one of the competitors to make a meta rinha program, which is an interpreter built in the Rinha language itself. In fact, the example above comes almost straight from the meta rinha program, I just changed the names. I used meta rinha to find bugs with closures in my interpreter. I think an interpreter that runs another interpreter is a good quick measure for correctness, everything else in the interpreter is simpler than closure handling. This is an example of a meta-rinha program:let fac = (ExpLet, (&quot;f&quot;, ( (ExpLambda, (&quot;x&quot;, (ExpIf, ((ExpVar, &quot;x&quot;), ((ExpMul, ((ExpVar, &quot;x&quot;), (ExpApp, ((ExpVar, &quot;f&quot;), (ExpSub, ((ExpVar, &quot;x&quot;), (ExpK, 1))))))), (ExpK, 1)))))), (ExpApp, ((ExpVar, &quot;f&quot;), (ExpK, 10))))));let _ = print(&quot;factorial 10: &quot; + second(eval(fac, empty_env)));My entryMy idea for Rinha was to build an initial simple reasonably optimized interpreter and then try to type-check it for AOT, but that didn‚Äôt work. I did learn a bit about Hindley Milner type inference, but the input language is just too dynamic, there are no type annotations, and it would need some Haskell-like type classes to make operator overloading work (i.e. the + does integer sum and string concat). That‚Äôs too complex for the timeframe I had, so I decided to stick with an interpreter. I wouldn‚Äôt have much time to dedicate to the Rinha before the competition as I was traveling, but I had been curious about an article I had read about a tree-walker-like interpreter, but on steroids. I did experiment with the approach in the past on a smaller scale, but I thought it was finally time to make it for realSome time ago, I read a Cloudflare article in which they built an interpreter for Wireshark filters to add it to their firewall solutions. Their first solution was a treewalker, which was alright‚Ä¶ it would be nice if they were a bit faster though. However, the risks and overheads of JITting the filters to all platforms are way too high, outweighing any benefits. They are processing untrusted network input, and doing low-level unsafe programming (like running JIT, generating machine code, marking sections of memory as executable, and so on) also didn‚Äôt seem like a good idea at the scale they deal with.Instead, they did a HOAS approach (according to Sofia). HOAS means High-Order Abstract Syntax, and I will not pretend I know what it means. My entry uses Rust, and I actually call it Lambda compilation, because it‚Äôs all lambdas. It‚Äôs all dynamic dispatch and closures. HOAS means you use the host language‚Äôs features to run the language, but it‚Äôs only true HOAS for the most simple of languages, like lambda calculus. For anything a bit more complicated it‚Äôs more difficult to pull it off in a pure HOAS fashion. ChatGPT tells me it‚Äôs actually a mixture of First-Order Abstract Syntax (FOAS) and HOAS, but I will also not pretend I know what exactly it means. Therefore, let‚Äôs go with lambda compilation, it describes exactly what happens in the code.Also, bear in mind that I won‚Äôt worry too much about memory leaks and memory consumption in general, this is a toy interpreter for a competition, not production usage. Still, I think some things I learned would work well in a production interpreter, while for others I exploited the nature of the competition and just did some hacky stuff to squeeze performance.For the lambda compilation approach, suppose we need to compile a binary expression that loads a constant value and a variable, I would need this code:pub type LambdaFunction = Box&amp;lt;dyn Fn(&amp;amp;mut ExecutionContext) -&amp;gt; Value&amp;gt;;// Enum for runtime valuespub enum Value { Int(i32), Bool(bool), Str(String), Tuple(Box&amp;lt;Value&amp;gt;, Box&amp;lt;Value&amp;gt;), Closure(...), //and so on}...fn compile(&amp;amp;self, ast: &amp;amp;Term) -&amp;gt; LambdaFunction { match ast { Term::Int { value } =&amp;gt; Box::new(move |exec_context| Value::Int(value)), Term::Var { name, .. } =&amp;gt; Box::new(move |exec_context| exec_context.cur_frame().variables.get(&amp;amp;name).unwrap().clone()) Term::BinaryExp { op, lhs, rhs } =&amp;gt; { let lhs_compiled = self.compile(lhs); let rhs_compiled = self.compile(lhs); match op { Operator::Plus =&amp;gt; Box::new(move |ec| { let lhs = lhs_compiled(ec); let rhs = rhs_compiled(ec); match (lhs, rhs) =&amp;gt; { Value::Int(l), Value::Int(r) =&amp;gt; Value::Int(l + r), //... also handles string and int concat } }), //... also handles other operators } } ... }}Instead of matching the AST at runtime, we try to pre-compile every decision that the tree walker would make. For instance,a regular treewalker would have no choice but to pattern-match every kind of operator (plus, multiply, minus, etc) at every execution of a binary operation. In our scenario, we already looked into it and determined we only need to do the dynamic type checking to ensure both sides are Int, and the operator + handler is returned directly.How much faster is this compared to a regular tree walker? Cloudflare claims it‚Äôs 10~15%, and their article also uses Rust, but the language is very different.The problem is: I didn‚Äôt measure it in Rust. However, I did measure it in Ocaml using ocamlopt, and I have this OCaml version of the Rinha interpreter that compares the two approaches. In general, I‚Äôm getting around 8% improvement using this approach. Doing it in OCaml was much more of a joy compared to Rust, but doing it in Rust wasn‚Äôt too difficult either. I‚Äôd say this lambda compilation strategy is a bit more complicated to debug though.So I wrote the initial implementation and tried to run the perf.rinha file, here it is in case you don‚Äôt want to scroll all the way back:let iter = fn (from, to, call, prev) =&amp;gt; { if (from &amp;lt; to) { let res = call(from); iter(from + 1, to, call, res) } else { prev }};let work = fn(x) =&amp;gt; { let work_closure = fn(y) =&amp;gt; { let xy = x * y; let tuple = (xy, x); let f = first(tuple); let s = second(tuple); f * s }; iter(0, 200, work_closure, 0)};let iteration = iter(0, 100, work, 0);print(iteration)This code ran in 43ms. That‚Äôs our baseline. It‚Äôs pretty bad. Let‚Äôs optimize it.Optimization 1: BTreeMap instead of HashMapIn the interpreter VM state, I store variables in the call frame data. Each time a function is called, I create a new call frame with an empty HashMap&amp;lt;&amp;amp;str, Value&amp;gt; map for the variables, and store all let bindings, closure, and params there.This first optimization is just changing that map to a BTreeMap&amp;lt;&amp;amp;str, Value&amp;gt; instead. Hashing strings is kinda expensive, so let‚Äôs stop doing that.After this change, we‚Äôre down to 28ms, a 49% improvement.Optimization 2: Use Vec instead of any kind of mapDuring compilation, we scan all variables in the program, and I actually have a process in which I intern their strings. I end up with a list of all strings used for variables in the program, like [iter, from, to, call, prev, res, work, work_closure, y, xy, tuple, f, s, iteration], 14 variables. Instead of strings, I have a usize number for each one, where iter=0, from=1, and so on. So this is how it worked: At compile time, I tell the call_function procedure to create a call stack with 14 slots Copy parameters and closures into these 14 slots, as available Whenever I need to fetch these values, I just tell which slot index I want. At compile time this becomes a simple stack_frame.variables[pos], no map lookup or hashing needed ???? ProfitThe result was‚Ä¶ underwhelming. Now we‚Äôre slower than before, 35ms. This is because each new frame eventually needs an allocation of size 14, and the size of Value at this point I think was around 24 bytes. This is a bit of foreshadowing so let‚Äôs keep going.Optimization 3: Use a separate Vec for each variable and track their evolution separatelyTo avoid every new frame needing a new big allocation, we need to make this process trivial. Therefore, I started tracking the variables in my global VM state:struct ExecutionContext { ... call_stack: Vec&amp;lt;CallFrame&amp;gt;, variables: Vec&amp;lt;Vec&amp;lt;Value&amp;gt;&amp;gt; ...}This way, variables became a 2D vector in which the first dimension identifies the variable, and the second dimension is just the variable‚Äôs ‚Äústack‚Äù. So instead of tracking frames of variables, I track‚Ä¶. variables of frames? That‚Äôs confusing, but I hope it makes some sense. Reminds me of column databases, where a table is not millions of rows broken apart into many cells for each field: The table has one big array for each of its fields, and each index into that field identifies a record.To access the variable from, one just needs to to self.variables[0].last().unwrap(), and new call frames just need to push new values onto the respective indices for each variable. When a stack frame ends, we need to pop the values that were used, therefore, the call frame tracks which variables were pushed:struct CallFrame { ... pushed_vars: Vec&amp;lt;usize&amp;gt;}When we pop a frame, we read the pushed_vars and pop each variable:fn pop_frame(&amp;amp;self) { let frame = self.call_stack.pop().unwrap(); for v in frame.pushed_vars { self.variables[v].pop() }}That optimization brings us to 14ms, the fastest yet. That‚Äôs a 204% improvement from baseline.Optimization 4: Reuse frame allocationsLet‚Äôs take a look at our fib function:let fib = fn (n) =&amp;gt; { if (n &amp;lt; 2) { n } else { fib(n - 1) + fib(n - 2) }};print(&quot;fib: &quot; + fib(10))This code does a lot of unnecessary work, allocates a lot of call frames, it‚Äôs incredibly wasteful. To keep track of pushed vars, we are still doing a lot of allocations and array resizing. What if we could reuse these allocated spaces?In the line fib(n - 1) + fib(n - 2), we will execute fib(n-1) which will allocate some frame data, and then dispose of the frame. But we could instead store that frame into a reusable_frames array, and at every new frame, we pop this array and clear the data inside the pushed_vars. This does not free the allocation, allowing us to reuse it. This helps on the perf.rinha benchmark too, as we execute the functions in a loop, their frames are also reused.This resulted in a 91% improvement from our last optimization, down to 7.4ms. We‚Äôre at a 482% improvement from baseline.Optimization 5: Non-heap tuplesLet‚Äôs take a look again at our Value enum:pub enum Value { Int(i32), Bool(bool), Str(String), Tuple(Box&amp;lt;Value&amp;gt;, Box&amp;lt;Value&amp;gt;), Closure(...), //and so on}Our perf.rinha creates a tuple, which results in 2 heap allocations. Value can be of any variant, and since using them directly would cause infinite recursion, they must be Box&amp;lt;Value&amp;gt; so I can have things like (1, (true, (&quot;foo&quot;, fn() {}))).Or do we ü§®? (cue in Vsauce music)Good news: perf.rinha allocates only (i32, i32) tuples. So here‚Äôs the idea:pub enum Value { Int(i32), Bool(bool), Str(String), IntTuple(i32, i32), BoolIntTuple(bool, i32), IntBoolTuple(i32, bool), BoolTuple(bool, bool), Tuple(Box&amp;lt;Value&amp;gt;, Box&amp;lt;Value&amp;gt;), ...}By creating these extra primitive-only tuples, we avoid talking with the allocator. This resulted in a 118% improvement in runtime, down to 3.4ms. We were at 7.4ms before. This is ~1170% improvement from baseline.However, this is where the low-hanging fruit ends. My final result will be 1.55ms, and the remaining 1.9ms will be difficult to shave off. But let‚Äôs take a detour for a moment, because it‚Äôs time to improve our support for recursive functions.Optimization 6: TrampolinesThis item will explain the implementation of Tail-Call optimization, but we won‚Äôt gain much performance yet. At this point in the competition, I was worried about recursion. Let‚Äôs take a look at the classical recursive Fibonacci algorithm again:let fib = fn (n) =&amp;gt; { if (n &amp;lt; 2) { n } else { fib(n - 1) + fib(n - 2) }};print(&quot;fib: &quot; + fib(10))If you translate this code to Rust or C and compile it using optimizations, it transforms this code into a loop. There is an optimization step in LLVM that transforms algorithms such as fib into their iterative form. It only does this sometimes, not always, because some constraints need to hold.This Fibonacci algorithm, as is, is not in tail call form. There‚Äôs nothing I can do to make it a tail call. I could look into what LLVM does but that would be too much for the time I had.However, in functional programming, you can just write your function in tail call form:let fib_tc = fn (n, a, b) =&amp;gt; { if (n == 0) { a } else { fib_tc(n - 1, b, a + b) }};Whenever the last thing a function does is a call to itself, we can actually reuse the stack frame by transforming it into this format:let fib_tc = fn (n, a, b) =&amp;gt; { if (n == 0) { a } else { fn () =&amp;gt; fib_tc(n - 1, b, a + b) //This became a function declaration }};In this case, the compiler can apply a pre-processing step on the AST and wrap the function call in another function instead of evaluating it immediately.Then at runtime, the interpreter does something like this, in pseudocode:let result = call(fib_tc, ...);while (is_callable(result)) { result = result();}return resultNotice that when we call fib_tc, we only create one stack frame. This stack frame is popped when the call ends, and then the while loop checks if the result is a callable value. If so, we call it in a new stack frame, but we popped the stack before so it won‚Äôt create a deep stack. The function returns (stack frame is popped), and so on. Due to this jumping behavior up and down in the stack, this technique is called a trampoline.This technique avoids the creation of a deep stack that potentially overflows. There is also CPS (Continuation-Passing Style) that can even handle the original recursive fib function, but they are more complex to implement. The trampoline technique will suffice for now.This resulted in no performance gain, but it‚Äôs a nice feature‚Ä¶ fear not, the trampoline will come in clutch in a few paragraphs.Optimization 7: Reduce the size of the ValueAt this point, here‚Äôs our full Value enum:pub struct Closure { pub callable_index: usize}pub enum Value { Int(i32), Bool(bool), Str(&amp;amp;&#39;static str), IntTuple(i32, i32), BoolIntTuple(bool, i32), IntBoolTuple(i32, bool), BoolTuple(bool, bool), DynamicTuple(Box&amp;lt;Value&amp;gt;, Box&amp;lt;Value&amp;gt;), Closure(Closure), Trampoline(Closure), Error(&amp;amp;&#39;static str),}There‚Äôs a problem: The value is a whopping 24 bytes in size. That‚Äôs bad. On a 64-bit computer, we can handle 8 bytes at a time, and we‚Äôre 3x larger than that.This enum is 24 bytes because it uses 1 byte for the tag, but the values inside the enum variant need to be aligned, so Rust determined an extra 7 bytes of padding is needed. Then we need enough space for the largest variant, which is DynamicTuple(Box&amp;lt;Value&amp;gt;, Box&amp;lt;Value&amp;gt;), used when tuples contain values other than Int or Bool. Each Box needs 8 bytes, so there are 16 bytes for the value, plus 8 bytes for the aligned tag.At the time I wrote the optimizations we‚Äôll see, I did not know about NaN tagging. NaN tagging is a technique where you represent every value as a NaN variant. Turns out there are many many IEE754 double values that are NaN, and we can use silent NaNs and manipulate their bits to indicate what kind of value they are.The benefit of NaN tagging comes when our language has support for double values. Whenever we need a double value, we can just use the value as-is. Rinha, however, has no doubles, only 32-bit signed integers, so the usefulness of NaN tagging is limited. However, NaN tagging also handles pointers: In most machines, pointers only really use 48 bits of information. Maybe in the future, I will implement something similar to NaN tagging. This technique should alleviate the impact of padding space that Rust adds to enums to ensure alignment, though it might create other overheads.Therefore, what I did was an emulation of 32-bit pointers in the most hacky way possible: Just indices on a Vec. My heap is really just a Vec. I store values in a Vec and use a u32 index instead of usize. You can allocate at most ~4 billion tuples, ~4 billion strings, and ~4 billion closures in my interpreter‚Ä¶ it‚Äôs a price we pay for performance. This is the new definition:pub struct Closure { pub callable_index: u32, pub closure_env_index: u32}pub struct HeapPointer(u32);pub struct StringPointer(u32);#[derive(Clone, Debug, Eq, PartialEq, PartialOrd, Ord)]pub enum Value { Int(i32), Bool(bool), Str(StringPointer), IntTuple(i32, i32), BoolIntTuple(bool, i32), IntBoolTuple(i32, bool), BoolTuple(bool, bool), DynamicTuple(HeapPointer, HeapPointer), Closure(Closure), Trampoline(Closure),}This is 12 bytes. Now Rust chose a 4-byte alignment due to HeapPointer and values inside Closure being 4 bytes. Notice I also got rid of the Error variant, it was useless. Now we can fit many more Values in the CPU cache and they are much faster to copy. Now we are down to 2.9ms from 3.4ms. 17% faster than the previous, and 1386% from baseline.Optimization 8: Reduce the size of the Value even moreThis is just an extension of the previous idea. Let‚Äôs get it down to 8 bytes by creating a ClosurePointer of 4 bytes.I also had to remove the extra tuple values and make all of them point to heap values, otherwise, the size of the enum would still be 12 bytes because i32 and bool align in 4 bytes.We‚Äôre at 2.5ms, 16% improvement over last, 1624% over baseline.Optimization 9: Frame reuse in TCOCurrently, our TCO implementation just avoids creating a physical call frame on every call inside the recursive function, but it still creates a virtual VM frame. This creates a lot of unnecessary allocations.In the end, I spent a whole afternoon making it work, but the implementation ended up being really simple: a flag on the call frame that tells whether we should reuse the fame. Then calling the function recursively, we read that flag and simply skip popping and pushing that frame. Only when the trampoliner finishes executing we really pop the frame.This reduced the runtime to 2.2ms, 13.6% faster than the previous, and 1859% over baseline.Optimization 10: Empty closuresSome functions don‚Äôt capture values outside of them. However, the interpreter still allocates a closure value for every function, including ones that are frequently called but capture nothing.When we compile a function, we store it in a vector and pass a slice of this vector to the execution context:pub struct Closure { pub callable_index: usize, pub closure_env_index: usize}pub struct Callable { pub parameters: &amp;amp;&#39;static [usize], pub closure_indices: &amp;amp;&#39;static [usize], pub body: LambdaFunction, pub trampoline_of: Option&amp;lt;usize&amp;gt;}pub struct ExecutionContext&amp;lt;&#39;a&amp;gt; { ... pub functions: &amp;amp;&#39;a [Callable], pub closures: Vec&amp;lt;Closure&amp;gt;, pub closure_environments: Vec&amp;lt;BTreeMap&amp;lt;usize, Value&amp;gt;&amp;gt;, ...}For closures, at this point, we are using BTreeMaps. Somewhere down the line I actually started using Vec, but I forgot to document this change. The usize key is the variable ID. Every time we declare a function, we create a new BTreeMap and store it in the vector, which makes the closure_environments grow rapidly. But there are many cases where this is not necessary: If the compiler analyzes the function and decides it doesn‚Äôt capture anything, we can just use one canonical ‚Äúcapture-less‚Äù function.Therefore, at the beginning of the execution during instantiation of the ExecutionContext, for every callable we store a Closure that points to a non-existent closure_env_index, like u32::MAX. This points to a value that doesn‚Äôt exist and would crash if used, but the fact that it tried to load a non-existing closure is a bug in our analysis step. let empty_closures = { let mut closures = vec![]; for (i, _) in program.functions.iter().enumerate() { closures.push(Closure { callable_index: i, closure_env_index: usize::MAX, }) } closures };Whenever we find a non-capturing function, we evaluate it as a Closure { callable_index, closure_env_index: u32::MAX } and return a ClosurePointer(callable_index). As you can see in the code above, the callable index points to the actual callable index in the functions vec.This entire thing improved the performance by‚Ä¶.. 0%. Nice :) Some benchmarks did run without exploding memory though, so that‚Äôs good.Optimization 11: Small TuplesFor tuples with int values that fit into i16, we don‚Äôt heap allocate them, instead, I created a SmallTuple(i16, i16) variant.Down to 2.1ms.The bug fixesSo many optimizations done, everything must be running just fine, right? Well‚Ä¶ it turns out I had some massive bugs regarding closures. Sometimes values weren‚Äôt being loaded from where they should be. Basically, something like this would happen:let y = 10;let f = fn() { y };let y = 20; //shadowed!print(y); //should print 10, not 20To my horror, it was printing 20 because values weren‚Äôt being copied into the closure properly. Unfortunately, after fixing it, the performance got much worse, up to 3.4ms. That‚Äôs a huge setback.However, the fix was quite easy.Optimization 12: Minimal function closuresOur capture analysis in Optimization 10 missed the opportunity to reduce the copies. The way closures were created was: Get all the variables in scope right now Copy them into the closure GGHowever, the closures usually need only a subset of these variables. Turns out I had that information in hand, so it was easy to use that.Down to 2.2ms, almost pre-bugfixing levels.This was the program I submitted for the competition.One more hiccup‚Ä¶This one got almost everyone by surprise. Sofia tested her benchmark program only to break almost all interpreters, and not for the reasons we expected.This is the code that they tried to run, which isn‚Äôt that big, having almost 400 lines of code. You can take a look at it here. Each let statement has a next node, which could point to another let, that has another next, and so on. In my case, Serde would just not parse that file. It complained about ‚Äúmaximum recursion depth‚Äù or something, so I had to change some configs and add some dependencies, and then I got afraid my compiler would crash due to excessive recursion. I thought they had thousands and thousands of lines of code.After changing some stuff (using lists of let expressions instead of a linked list of let), I fixed the issue but lost a bit of performance, now the program runs in 2.3ms. I would only understand why a few days later, but at this time, I had to quickly submit a fix.At this point, my interpreter was running fib(46) in around 220 seconds, while the winning submission from Raphael was running it in 140 seconds without memoization. Ah yes, some people implemented memoization, to varying levels of success. Raphael did too, but this 140s number was acquired when his interpreter didn‚Äôt have memoization.The next optimizations were done after the competition.Optimization 13: Undo optimization 3In Optimization 3, we did that weird variable stack tracking that resembled, IMO, a column-based database. I decided to get rid of it, and similar to Optimization 12, compute exactly how much space the stack frame needs. I had the information available. Then I changed the Call frame to store a Vec&amp;lt;Value&amp;gt; inside it. We got down to 2.0ms from 2.3ms, which is the fastest so far. But this revealed some subtle bugs that I had to fix, so it‚Äôs also the fastest and most correct implementation at this point.The next 2 implementations were very easy and had massive effects.Optimization 14: Solving the pre-submission slowdown.What happened in the pre-submission bugfix?Well, let statements became vectors instead of linked lists from the AST format. Because of that, bodies of functions and if statements had lists of expressions that had to be run like this:let all_statements = self.compile_exprs(body);Box::new(move |ec, frame| { let mut last = None; for l in leaked { last = Some(l(ec, frame)); } last.unwrap()});It turns out this whole looping machinery causes a lot of overhead when we only have one simple expression that, in the end, just returns a variable or a literal value. If only we knew how many statements we had in the body, we could do something about it‚Ä¶fn join_lambdas(&amp;amp;mut self, mut lambdas: Vec&amp;lt;LambdaFunction&amp;gt;) -&amp;gt; LambdaFunction { if lambdas.len() == 1 { return lambdas.pop().unwrap(); } Box::new(move |ec| { let mut last = None; for l in leaked { last = Some(l(ec)); } last.unwrap() });}With this change, perf.rinha barely changed, from 2.0ms to 1.9ms. However, fib(30) went from 110ms down to 77ms(!). This is a massive gain.Other loop-heavy code also got significantly faster. For instance, this code that counts to 2 billion:let loop = fn (i, s) =&amp;gt; { if (i == 0) { s } else { loop(i-1, s+1) }};print(loop(2000000000, 0))This used to run in 60 seconds and now runs in 47 seconds, rivaling the fastest interpreter among the people in the Discord chat (the 7th place submission from fabiosvm).Final optimization: Pass the call frame alongWhenever we build a lambda function, we do this: Box::new(move |ec| { ec.frame().stack_data... /*do something with frame data*/ })The ec.frame() call returns the top of the call stack, which is stored in a Vec inside the execution context. However, why do this when the call stack is quite explicit in our program? Why save in a vector, which needs to do a size check for every push even if enough memory exists?Turns out we can just do this:Box::new(move |ec, frame| { frame.stack_data... /*do something with frame data*/ }) ÀÜÀÜÀÜÀÜÀÜ new parameterJust pass the frames along. When we call a function, we create the new frame and pass it to the callee‚Äôs compiled LambdaFunction.This results in a massive improvement.perf.rinha now runs in 1.55ms, down from 1.95ms. This is where I achieved the 27x speedup. fib(46) runs in 120s from 190ms of the last optimization and the big looping function that counts to a billion now runs in about 35 seconds instead of a full minute.I think if we ran the benchmark again I would have some chance of getting a better position ;)ConclusionI had a blast working on Rinha. At some points, I had to fire up callgrind and cachegrind to see what was going on, had to do a bunch of experiments that ended up not working, but it was fun experimenting with different techniques. Even if I didn‚Äôt win the competition, I had a lot of fun programming and talking with people on Discord and exchanging ideas.Maybe the next challenge is to fully type-check the language and use LLVM, who knows?Or maybe I should just continue working on my other compiler project, Donkey, which is a statically typed (with type annotations) Python-like programming language that tastes like C with some C++-template-inspired generics. Or maybe rest a little bit :)" }, { "title": "Deploying an ASP.NET gRPC service on GKE with GCE Ingress", "url": "/posts/kubernetes-gke-grcp-aspnet/", "categories": "Kubernetes, GKE, GCE", "tags": "kubernetes gke gce", "date": "2021-12-11 03:00:00 -0300", "snippet": "ContextAt Monomyto Game Studios, we use Kubernetes to manage our infrastructure. Our backend team consists, currently, of just one person (me), so I‚Äôd like to keep things as simple as possible, while still having a way to define all of our infrastructure in a single place, and make it deployable quickly using kubectl. Our development team is mostly composed of Unity programmers, some of them having done a little bit of backend development but not much.Our game, Gunstars, has a gRPC service written in C# and ASP.NET. We also use MagicOnion as an abstraction layer on top of gRPC, as it works well on Unity and makes it possible to simply define interfaces instead of writing .proto files. In order to manage and scale this application, we use Kubernetes. Specifically, we decided to deploy it on GKE (Google Kubernetes Engine). Our requirements for this application are: Handle realtime communications Manage player sessions Manage squads Perform matchmaking functions Manage friend requestsAs you might know, gRPC uses HTTPS/2 under the hood to establish a bidirectional communications channel. This application is exposedto the internet, and therefore needs HTTPS to make the communication secure.Configuring HTTPS + HTTP/2For a simple service that does not need HTTPS, we can simply use a LoadBalancer service on kubernetes:apiVersion: v1kind: Servicemetadata: name: gunstars-realtime-service labels: app: gunstars-realtime-servicespec: ports: - port: 5000 targetPort: 5000 selector: app: gunstars-realtime type: LoadBalancerThe load balancer will be exposed to the internet, and GKE will automatically assign a public IP to this service. However, our options are limited here in terms of HTTPS: There seems to be no way to configure the load balancer with this kind of service.Thankfully, we have another kind of resource on kubernetes: Ingress. An external ingress also exposes a service to the internet, but we have more options to play with. Let‚Äôs take a look at our deployment first:# Some details ommited for brevityapiVersion: apps/v1kind: Deploymentmetadata: name: gunstars-realtime labels: app: gunstars-realtimespec: replicas: 1 selector: matchLabels: app: gunstars-realtime template: metadata: labels: app: gunstars-realtime spec: containers: - name: gunstars-realtime image: &quot;...&quot; imagePullPolicy: &quot;Always&quot; env: - name: ASPNETCORE_URLS value: &quot;http://+:5000;http://+:5001&quot; ports: - containerPort: 5000 - containerPort: 5001This deployment exposes 2 ports: 5000 and 5001. 5000 will be used for the gRPC HTTP/2 connections, while 5001 is only for health checking and will not be exposed to the internet. Notice that our ASPNETCORE_URLS also configures some HTTP listeners. This is where my mistake lives, as you‚Äôll soon find out.Now it‚Äôs time to define our service. Let‚Äôs take a look:apiVersion: v1kind: Servicemetadata: annotations: cloud.google.com/app-protocols: &#39;{&quot;grpc&quot;:&quot;HTTP2&quot;, &quot;health&quot;:&quot;HTTP&quot;}&#39; cloud.google.com/backend-config: &#39;{&quot;default&quot;: &quot;gunstars-realtime-healthcheck&quot;}&#39; cloud.google.com/neg: &#39;{&quot;ingress&quot;: true, &quot;exposed_ports&quot;: {&quot;5000&quot;:{}}}&#39; name: gunstars-realtime-service labels: app: gunstars-realtime-servicespec: ports: - port: 5000 targetPort: 5000 protocol: TCP name: grpc - port: 5001 targetPort: 5001 protocol: TCP name: health selector: app: gunstars-realtime type: NodePortNotice that we define the ports of the service. The cloud.google.com/app-protocols annotation lets us define the protocol for each port, which have been named as grpc or health.Also notice the cloud.google.com/backend-config annotation: we need to tell GKE how to perform health checking on our service, otherwise the backend will report its status as NOT_READY or UNKNOWN. We also explicitly tell which ports are exposed. Maybe some of this configuration isn‚Äôt really necessary, but I haven‚Äôt tested yet.Our backend config looks like this:apiVersion: cloud.google.com/v1kind: BackendConfigmetadata: name: gunstars-realtime-healthcheckspec: healthCheck: checkIntervalSec: 15 port: 5001 type: HTTP requestPath: /health connectionDraining: drainingTimeoutSec: 15Ingress definitionTo define an ingress, we have a choice to make. We can use a load balancer such as Nginx, or use Google‚Äôs load balancer. Each one has its pros and cons: Nginx has tons of configurations to choose and is very flexible, while GCE Load balancer is not very configurable. However, due to the size and inexperience of the team regarding backend services, the tradeoff I‚Äôm choosing is letting GCE manage most things. The backend is a responsability of the entire company, and everyone should have some knowledge about how it works. Linking a certificate with Nginx automatically would require a ton of configuration with cert-manager.io, which would use Letsencrypt to generate a certificate, and renew it. However, the amount of configuration needed can be overwhelming. Therefore, we chose GCE‚Äôs load balancer (GCLB) which is the default load balancer on GKE.Therefore, we can just declare a ManagedCertificate like this:apiVersion: networking.gke.io/v1kind: ManagedCertificatemetadata: name: gunstars-realtime-certspec: domains: - ...(ommited)...This certificate can be linked to the ingress:apiVersion: networking.k8s.io/v1kind: Ingressmetadata: name: gunstars-realtime-https-ingress annotations: networking.gke.io/managed-certificates: &quot;gunstars-realtime-cert&quot; kubernetes.io/ingress.global-static-ip-name: gunstars-realtime-ingress #a reserved IPspec: defaultBackend: service: name: gunstars-realtime-service-https port: number: 5000And success! Everything should work, right?Incorrect assumptions‚Ä¶ and there goes 3 or 4 days investigating why it doesnt. Turns out that, whenever I sent a request to the backend, I would be met with a 502 error, while the access logs on the Ingress would just say failed_to_connect_to_backend. Why would that be the case?I tried deploying an HTTP1.1 application, it worked fine. I tried using the gcr.io/google-containers/echoserver to test HTTP/2 specifically, it worked fine. What gives?In my experience, the applications inside our network (behind a load balancer) can just listen to unencrypted traffic, while the load balancer takes care of HTTPS. It seems like this works for HTTP 1.1, but GKE is quite picky regarding HTTP/2. It is assumed that HTTP/2 will always use HTTPS, but that‚Äôs not entirely true: it is possible to use unencrypted HTTP/2 (for instance, when testing things locally).Let‚Äôs take a look at our deployment again, specifically our environment variables:env: - name: ASPNETCORE_URLS value: &quot;http://+:5000;http://+:5001&quot;That seems to be following what‚Äôs been true in my experience. My ASP.NET application is listening on port 5000 unencrypted.However, for some reason, that doesn‚Äôt work: we need to use HTTPS in the deployment as well! So I changed our deployment to this:env: - name: ASPNETCORE_URLS value: &quot;https://+:5000;http://+:5001&quot; - name: ASPNETCORE_Kestrel__Certificates__Default__Path value: &quot;./https/aspnetapp.pfx&quot; - name: ASPNETCORE_Kestrel__Certificates__Default__Password value: &quot;...&quot;Now Kestrel is listening on 5000 for HTTPS traffic, and I just used the development certificate I already had in hand.GCLB will not check whether the certificate is valid or not.I also defined the URLs in the appsettings.json file:{ &quot;Kestrel&quot;: { &quot;Endpoints&quot;: { &quot;Grpc&quot;: { &quot;Url&quot;: &quot;https://0.0.0.0:5000&quot;, &quot;Protocols&quot;: &quot;Http2&quot; }, &quot;Http&quot;: { &quot;Url&quot;: &quot;http://0.0.0.0:5001&quot;, &quot;Protocols&quot;: &quot;Http1&quot; } } }}And that works!ConclusionAfter googling for 3 days on and off for a solution to failed_to_connect_to_backend, I couldn‚Äôt find anything. Everything seemed to indicate that unencrypted traffic to the backend should work. However, I actually didn‚Äôt read things carefully. For instance:GCP HTTPS LB health check is working but no real traffic getting in, what could explain this?GCP Load Balancer: 502 Server Error, ‚Äúfailed_to_connect_to_backend‚ÄùOn these stackoverflow posts, it now seems to be implied that the deployment (or backend instances) should be listening for HTTPS, encrypted traffic.Other posts are seemingly more thorough, but do not address the issue at all:Server behind GCP Load Balancer occalionally gets 502 Server Error, ‚Äúfailed_to_connect_to_backend‚ÄùWhen you do things in a rush and have to do many context switches in your day, keeping track of everything and give that much attention to detail to what people write can be hard. I think this should be better documented in the GKE guides. As someone who is responsible for the backend services but isn‚Äôt that much familiar with proxies and load balancing configuration, this can be an unnecessarily time consuming endeavor.Maybe if I had read things more carefully I would have realized this before. But if you‚Äôre having the same problem as me (failed_to_connect_to_backend), there you go: just do HTTPS all the way, and use a self-signed certificate on the deployment.Another possible fix is to use nginx, which offers far more flexibility and tons of configurations, and cert-manager.io to automatically apply SSL certificates to the ingress, just be aware that this can be overwhelming for a small and inexperienced team.I hope no one wastes 3 days on this. Have a good day!" } ]
