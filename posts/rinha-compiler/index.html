<!DOCTYPE html><html lang="en" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="day-prompt" content="days ago"><meta name="hour-prompt" content="hours ago"><meta name="minute-prompt" content="minutes ago"><meta name="justnow-prompt" content="just now"><meta name="generator" content="Jekyll v4.3.2" /><meta property="og:title" content="How I improved my interpreter speed by 27x - A Compiler/Interpreter Competition" /><meta name="author" content="Ricardo Pieper" /><meta property="og:locale" content="en" /><meta name="description" content="In September 2023, one of the most anticipated events in Brazil took place: the “Rinha de Compiladores”. This is translated roughly to “Compiler Battle”, “Compiler Bout”, or “Compiler Fight”. If you don’t speak Portuguese, “rinha” is pronounced like “rinya” and not “rin haa”, the nh is pronounced like ñ in Spanish." /><meta property="og:description" content="In September 2023, one of the most anticipated events in Brazil took place: the “Rinha de Compiladores”. This is translated roughly to “Compiler Battle”, “Compiler Bout”, or “Compiler Fight”. If you don’t speak Portuguese, “rinha” is pronounced like “rinya” and not “rin haa”, the nh is pronounced like ñ in Spanish." /><link rel="canonical" href="https://ricardopieper.github.io//posts/rinha-compiler/" /><meta property="og:url" content="https://ricardopieper.github.io//posts/rinha-compiler/" /><meta property="og:site_name" content="Ricardo Pieper" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2023-10-17T12:00:00-03:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="How I improved my interpreter speed by 27x - A Compiler/Interpreter Competition" /><meta name="twitter:site" content="@ricardopieper1" /><meta name="twitter:creator" content="@Ricardo Pieper" /><meta name="google-site-verification" content="google_meta_tag_verification" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Ricardo Pieper"},"dateModified":"2023-11-07T22:31:30-03:00","datePublished":"2023-10-17T12:00:00-03:00","description":"In September 2023, one of the most anticipated events in Brazil took place: the “Rinha de Compiladores”. This is translated roughly to “Compiler Battle”, “Compiler Bout”, or “Compiler Fight”. If you don’t speak Portuguese, “rinha” is pronounced like “rinya” and not “rin haa”, the nh is pronounced like ñ in Spanish.","headline":"How I improved my interpreter speed by 27x - A Compiler/Interpreter Competition","mainEntityOfPage":{"@type":"WebPage","@id":"https://ricardopieper.github.io//posts/rinha-compiler/"},"url":"https://ricardopieper.github.io//posts/rinha-compiler/"}</script><title>How I improved my interpreter speed by 27x - A Compiler/Interpreter Competition | Ricardo Pieper</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="Ricardo Pieper"><meta name="application-name" content="Ricardo Pieper"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1.1.0/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } let self = this; /* always follow the system prefers */ this.sysDarkPrefers.addEventListener("change", () => { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.updateMermaid(); }); } /* constructor() */ get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode === ModeToggle.DARK_MODE; } get isLightMode() { return this.mode === ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer)) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } setDark() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_KEY); sessionStorage.removeItem(ModeToggle.MODE_KEY); } updateMermaid() { if (typeof mermaid !== "undefined") { let expectedTheme = (this.modeStatus === ModeToggle.DARK_MODE? "dark" : "default"); let config = { theme: expectedTheme }; /* re-render the SVG › <https://github.com/mermaid-js/mermaid/issues/311#issuecomment-332557344> */ $(".mermaid").each(function() { let svgCode = $(this).prev().children().html(); $(this).removeAttr("data-processed"); $(this).html(svgCode); }); mermaid.initialize(config); mermaid.init(undefined, ".mermaid"); } } } /* ModeToggle */ const toggle = new ModeToggle(); function flipMode() { if (toggle.hasMode) { if (toggle.isSysDarkPrefer) { if (toggle.isLightMode) { toggle.clearMode(); } else { toggle.setLight(); } } else { if (toggle.isDarkMode) { toggle.clearMode(); } else { toggle.setDark(); } } } else { if (toggle.isSysDarkPrefer) { toggle.setLight(); } else { toggle.setDark(); } } toggle.updateMermaid(); } /* flipMode() */ </script><body data-spy="scroll" data-target="#toc"><div id="sidebar" class="d-flex flex-column align-items-end" lang="en"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src="/assets/img/profilepic.png" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">Ricardo Pieper</a></div><div class="site-subtitle font-italic">Backend software engineer</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <button class="mode-toggle btn" aria-label="Switch Mode"> <i class="fas fa-adjust"></i> </button> <span class="icon-border"></span> <a href="https://github.com/ricardopieper" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href="https://twitter.com/ricardopieper1" aria-label="twitter" target="_blank" rel="noopener"> <i class="fab fa-twitter"></i> </a></div></div><div id="topbar-wrapper" class="row justify-content-center topbar-down"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>How I improved my interpreter speed by 27x - A Compiler/Interpreter Competition</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="core-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>How I improved my interpreter speed by 27x - A Compiler/Interpreter Competition</h1><div class="post-meta text-muted"><div> By <em> <a href="https://github.com/ricardopieper">Ricardo Pieper</a> </em></div><div class="d-flex"><div> <span> Posted <em class="timeago" date="2023-10-17 12:00:00 -0300" data-toggle="tooltip" data-placement="bottom" title="Tue, Oct 17, 2023, 12:00 PM -0300" >Oct 17</em> </span> <span> Updated <em class="timeago" date="2023-11-07 22:31:30 -0300 " data-toggle="tooltip" data-placement="bottom" title="Tue, Nov 7, 2023, 10:31 PM -0300" >Nov 7</em> </span> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="7074 words"> <em>39 min</em> read</span></div></div></div><div class="post-content"><p>In September 2023, one of the most anticipated events in Brazil took place: the “Rinha de Compiladores”. This is translated roughly to “Compiler Battle”, “Compiler Bout”, or “Compiler Fight”. If you don’t speak Portuguese, “rinha” is pronounced like “rinya” and not “rin haa”, the <code class="language-plaintext highlighter-rouge">nh</code> is pronounced like <code class="language-plaintext highlighter-rouge">ñ</code> in Spanish.</p><p>This is part of an ongoing series of Rinhas. The previous one was a backend-focused competition to see which implementation of a simple HTTP API was the fastest.</p><p>As of the time of writing this text, the Rinha Frontend is taking place, where the goal is to render a huge JSON with many thousands of elements in a tree-viewer-like structure in the least amount of time. The compiler rinha was actually more of an interpreter one, as there were very few compilers (or transpilers).</p><p>The masterminds of the competition are <a href="https://github.com/aripiprazole">Gabrielle</a> and <a href="https://github.com/algebraic-sofia">Sofia</a>. Gabrielle is a 17-year-old girl and Sofia is 21. When I learned about this, I could only remind myself of the time I was 17 years old, playing Counter-Strike all day, playing RF Online, and maybe allegedly probably eating spoonfuls of dirt. This was the general feeling of the Rinha Discord server.</p><p>Therefore, although I have some criticism about how the benchmarks were made, what they pulled off is nothing short of incredible. Hundreds of people engaged in the competition. Some of them had never written an interpreter. Now they see this is not some sort of crazy magical code. It’s just regular everyday code, as Jonathan Blow says.</p><p>Some implementations were participating only for the memes, and couldn’t care less about performance. They were designed to “provide the highest joy to the developer”, as did <a href="https://github.com/tiagosh/garbash">this person</a>. This interpreter was made in Bash and definitely deserves the Most Based Implementation prize, but unfortunately, there was no such category.</p><p>I ended up not winning the competition. This 27x perf improvement mentioned in the title is compared to my initial implementation, and that 27x only happened after the Rinha finished. My submission only had an 18x improvement over the initial implementation. I admit it’s a bit of a clickbait title, but all will be clear in the end. You can check the results by <a href="https://github.com/aripiprazole/rinha-de-compiler/blob/main/TESTS.md">clicking here</a>.</p><p>I ended up in 18th out of almost 200 submissions (only about 70 working implementations though lol). However, the benchmarking process had some hiccups and was done in a bit of a rush as the organizers admitted. It could very well be that, if the benchmarks were done differently, I could end up even lower in the ranks, but I hope I would finish in a better position.</p><p>Some really good implementations, including some of the fastest ones, did not end up in the top 3. I remember the fastest interpreter made in C following the excellent Crafting Interpreters book finished in 7th place. Some even used LLVM and could not hit the top 5. However, I am happy that the winner Raphael Victal actually had a very good implementation, though I’m biased because we had some similar ideas. My interpreter ended up with a score of around 30% less than him, and my interpreter was in fact around 30% slower than his.</p><p>While I am a bit disappointed with the results, the reality is I shouldn’t even be on that board, as my submission wouldn’t even build 3 minutes before the scores were revealed. Sofia was gracious enough to quickly run my fixed build moments before the reveal livestream, but I think I should’ve been disqualified.</p><p>An interpreter usually has 3 phases, as I’m gonna explain in the next paragraphs. Bear in mind that for the Rinha language, we didn’t need to write a parser, as the organizers provided a Rust program that would parse the language and spit out the JSON AST. If you don’t know what those words mean, don’t worry, the next paragraphs explain it.</p><h1 id="interpreter-phases">Interpreter phases</h1><p>To run code, we don’t simply get input text and start running/compiling it. Maybe in the past, some languages actually did it because resources were so scarce, but nowadays this is not the case. Interpreters and compilers usually have multiple steps and each one gives more and more structure to the input program.</p><h2 id="lexer">Lexer<a href="#lexer"><i class="fas fa-hashtag"></i></a></h2></h2><p>The lexer is the simplest part of the interpreter: it takes input strings and returns a stream/list of tokens.</p><p>In the Rinha language, function declarations look like this (I’ll be using JS syntax highlighting for Rinha examples):</p><div class="language-js highlighter-rouge"><div class="code-header"> <span label-text="JavaScript"><i class="fas fa-code small"></i></span> <button aria-label="copy" title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre><span class="kd">let</span> <span class="nx">add</span> <span class="o">=</span> <span class="nf">fn</span><span class="p">(</span><span class="nx">x</span><span class="p">)</span> <span class="o">=&gt;</span> <span class="p">{</span> <span class="nx">x</span> <span class="o">+</span> <span class="mi">1</span> <span class="p">};</span>
</pre></table></code></div></div><p>This is, of course, a function that just returns its parameter. A tokenizer could read this and return a list of the following tokens:</p><div class="language-rust highlighter-rouge"><div class="code-header"> <span label-text="Rust"><i class="fas fa-code small"></i></span> <button aria-label="copy" title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
</pre><td class="rouge-code"><pre><span class="p">[</span>
  <span class="nn">Token</span><span class="p">::</span><span class="n">LetKeyword</span><span class="p">,</span>
  <span class="nn">Token</span><span class="p">::</span><span class="nf">Identifier</span><span class="p">(</span><span class="s">"add"</span><span class="p">),</span>
  <span class="nn">Token</span><span class="p">::</span><span class="nf">Operator</span><span class="p">(</span><span class="nn">Operator</span><span class="p">::</span><span class="n">Assign</span><span class="p">),</span>
  <span class="nn">Token</span><span class="p">::</span><span class="n">FunctionDeclarationKeyword</span><span class="p">,</span>
  <span class="nn">Token</span><span class="p">::</span><span class="n">OpenParen</span><span class="p">,</span>
  <span class="nn">Token</span><span class="p">::</span><span class="nf">Identifier</span><span class="p">(</span><span class="s">"x"</span><span class="p">),</span>
  <span class="nn">Token</span><span class="p">::</span><span class="n">ClosedParen</span><span class="p">,</span>
  <span class="nn">Token</span><span class="p">::</span><span class="n">OpenCurlyBraces</span><span class="p">,</span>
  <span class="nn">Token</span><span class="p">::</span><span class="nf">Identifier</span><span class="p">(</span><span class="s">"x"</span><span class="p">),</span>
  <span class="nn">Token</span><span class="p">::</span><span class="nf">Operator</span><span class="p">(</span><span class="nn">Operator</span><span class="p">::</span><span class="n">Plus</span><span class="p">),</span>
  <span class="nn">Token</span><span class="p">::</span><span class="nf">IntLiteral</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
  <span class="nn">Token</span><span class="p">::</span><span class="n">CloseCurlyBraces</span>
<span class="p">]</span>
</pre></table></code></div></div><p>In this case, the only string we have is in identifiers. We could have more strings if I used a string literal somewhere. Identifiers are used for variable names, function names, parameter names, type names, and everything that identifies something in the program. As for the other tokens, I think they are self-explanatory.</p><p>That’s all a lexer really does. We are now ready for parsing.</p><h2 id="parser">Parser<a href="#parser"><i class="fas fa-hashtag"></i></a></h2></h2><p>In 2023, only a madman would be satisfied with just the tokens, and then just interpret them as is. We need more structure. For that, we build an AST, an abstract syntax tree. This process is so mechanical and boring that there are programs you can use to generate parsers for a language. You describe the grammar, and the language makes a parser for you. I’m not a big fan, but I do see why people use them. Some of these tools are ANTLR, Bison, Flex, LALRPOP, and others. You can also write your own Recursive Descent parser, which is described in the Crafting Interpreters book. I really recommend reading this book as it will give you the recipe to build an interpreter…. but I also have to say: It’s a lot of fun if you write one in your own way. Then consult the literature to see which mistakes you made. You will never forget how dumb your ideas actually are, and you won’t forget about the better idea in the book or even tricks you didn’t realize could be done.</p><p>Anyway, at the end of parsing, you’ll end up with a structure similar to this:</p><div class="language-rust highlighter-rouge"><div class="code-header"> <span label-text="Rust"><i class="fas fa-code small"></i></span> <button aria-label="copy" title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
</pre><td class="rouge-code"><pre><span class="n">LetExpression</span> <span class="p">{</span>
  <span class="n">name</span><span class="p">:</span> <span class="s">"add"</span><span class="p">,</span>
  <span class="n">expression</span><span class="p">:</span>
    <span class="n">FunctionDeclaration</span> <span class="p">{</span>
      <span class="n">parameters</span><span class="p">:</span> <span class="p">[</span><span class="s">"x"</span><span class="p">],</span>
      <span class="n">body</span><span class="p">:</span> <span class="p">{</span>
        <span class="nf">BinaryOp</span><span class="p">(</span>
          <span class="n">op</span><span class="p">:</span> <span class="nn">Operator</span><span class="p">::</span><span class="n">Plus</span><span class="p">,</span>
          <span class="n">lhs</span><span class="p">:</span> <span class="nf">Variable</span><span class="p">(</span><span class="s">"x"</span><span class="p">),</span>
          <span class="n">rhs</span><span class="p">:</span> <span class="nf">IntLiteral</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="p">)</span>
      <span class="p">}</span>
    <span class="p">}</span>
<span class="p">}</span>

</pre></table></code></div></div><p>The astute reader will understand how each piece of this representation relates back to the source code. For Rinha, we also had location information for everything, so what we knew where exactly in the source code each piece was declared.</p><h2 id="post-parsing">Post-parsing<a href="#post-parsing"><i class="fas fa-hashtag"></i></a></h2></h2><p>After you parse the input text, you have so many paths you can follow I can’t describe every single one in length, so here’s a short description. If you know what tree-walking, bytecode VM, JIT, or AOT mean, you can skip this part.</p><ul><li><strong>Tree-walking interpreter</strong>: Just recursively walk through the AST and execute the nodes. Along the way, you have to store the currently declared variables and functions. For Rinha, when you find a function, you can evaluate the function as a Function value, just like any other kind of value. For functions, you can store the entire AST data in this value, the parameter names, and a copy of the current environment (like declared variables) that will act as the function closure. This is a bit slow and you can definitely optimize things out, as I’ll show in the rest of the article.<li><strong>Bytecode VM</strong>: Do you know Java? Do you know Javascript? Maybe Python? All of those contain a Virtual Machine (VM) that executes what’s called a bytecode, which is a list of instructions similar to Assembly, but for a virtual machine (and not a physical machine like your CPU). The bytecode is designed in tandem with the virtual machine so that the VM can execute the bytecode efficiently. You can think of your physical processor as a physical machine that interprets some ““bytecode”” (machine instructions) and all of that happens in silicon, in hardware. The bytecode VM tries to emulate some of the things the CPU and OS do, like decoding instructions, doing arithmetic, branching (if statements), call stacks, and so on. This tends to be a lot faster than tree-walking interpreters because there’s less pointer chasing and better data locality: the AST is a tree structure often with many things allocated all over the heap. Bytecode on the other hand is a more compact representation that fits much better in CPU cache. One of the main overheads of interpreting code is determining “what to do next?”, and having a compact representation of the code in the CPU cache helps a lot. It also turns out my approach of HOAS, or what I call Lambda compilation (because it’s all boxed lambda functions) also helps with that.<li><strong>JIT (Just In Time) compilers</strong>: In comparison with native code, bytecode still leaves a lot of performance on the table. A simple instruction in an interpreted language takes many dozens of machine code instructions on a CPU. These instructions would take care of fetching data, moving the instruction pointer, managing the call stack, and so on. These things are emulated in software, which is going to be slower than running machine code on a physical CPU. But interpreted languages do come with a lot of flexibility and portability. What JIT compilers do is to get the best of the two worlds: at its core, the language is interpreted, but parts of the user’s code are compiled down to fast machine code. Sometimes the compiler can figure all of the types of a function even in languages without type annotations (like JS). This is why Javascript is so fast: what V8, JSC, and Spidermonkey do is nothing short of a miracle, and that’s due to JIT. When needed and possible, the JIT can generate native code for a function. The same compiled binary or JS file can run in any browser on any computer while retaining really good, sometimes near-native performance. Java and C# are languages that run in a JIT compiler (JVM and RyuJIT respectively, and more recently Java also runs in GraalVM).<li><strong>AOT (Ahead Of Time) compilers</strong>: This is what Rust, C++, C, Go, and many other languages do. They compile to native code from the beginning. No bytecode VM needed, and you get native performance as a result. You lose the ability to run the same binary in multiple OSs and CPUs, but you gain a lot of performance. Each OS and CPU architecture needs a different binary.</ul><h2 id="the-rinha-language">The Rinha Language<a href="#the-rinha-language"><i class="fas fa-hashtag"></i></a></h2></h2><p>Let’s take a look at the Rinha language for a bit to understand how to run it.</p><div class="language-js highlighter-rouge"><div class="code-header"> <span label-text="JavaScript"><i class="fas fa-code small"></i></span> <button aria-label="copy" title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
</pre><td class="rouge-code"><pre><span class="kd">let</span> <span class="nx">fib</span> <span class="o">=</span> <span class="nf">fn </span><span class="p">(</span><span class="nx">n</span><span class="p">)</span> <span class="o">=&gt;</span> <span class="p">{</span>
  <span class="k">if </span><span class="p">(</span><span class="nx">n</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">)</span> <span class="p">{</span>
    <span class="nx">n</span>
  <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
    <span class="nf">fib</span><span class="p">(</span><span class="nx">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="nf">fib</span><span class="p">(</span><span class="nx">n</span> <span class="o">-</span> <span class="mi">2</span><span class="p">)</span>
  <span class="p">}</span>
<span class="p">};</span>

<span class="nf">print</span><span class="p">(</span><span class="dl">"</span><span class="s2">fib: </span><span class="dl">"</span> <span class="o">+</span> <span class="nf">fib</span><span class="p">(</span><span class="mi">10</span><span class="p">))</span>
</pre></table></code></div></div><p>Rinha is a small dynamically typed functional language. Functions support recursion and you can declare functions inside functions, return functions from functions, take functions by a parameter that, when executed, returns new functions, and all that good stuff from functional programming.</p><p>I also wrote a small program that tests some of the language features, except for strings:</p><div class="language-js highlighter-rouge"><div class="code-header"> <span label-text="JavaScript"><i class="fas fa-code small"></i></span> <button aria-label="copy" title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
</pre><td class="rouge-code"><pre><span class="kd">let</span> <span class="nx">iter</span> <span class="o">=</span> <span class="nf">fn </span><span class="p">(</span><span class="k">from</span><span class="p">,</span> <span class="nx">to</span><span class="p">,</span> <span class="nx">call</span><span class="p">,</span> <span class="nx">prev</span><span class="p">)</span> <span class="o">=&gt;</span> <span class="p">{</span>
  <span class="k">if </span><span class="p">(</span><span class="k">from</span> <span class="o">&lt;</span> <span class="nx">to</span><span class="p">)</span> <span class="p">{</span>
    <span class="kd">let</span> <span class="nx">res</span> <span class="o">=</span> <span class="nf">call</span><span class="p">(</span><span class="k">from</span><span class="p">);</span>
    <span class="nf">iter</span><span class="p">(</span><span class="k">from</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="nx">to</span><span class="p">,</span> <span class="nx">call</span><span class="p">,</span> <span class="nx">res</span><span class="p">)</span>
  <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
    <span class="nx">prev</span>
  <span class="p">}</span>
<span class="p">};</span>

<span class="kd">let</span> <span class="nx">work</span> <span class="o">=</span> <span class="nf">fn</span><span class="p">(</span><span class="nx">x</span><span class="p">)</span> <span class="o">=&gt;</span> <span class="p">{</span>
  <span class="kd">let</span> <span class="nx">work_closure</span> <span class="o">=</span> <span class="nf">fn</span><span class="p">(</span><span class="nx">y</span><span class="p">)</span> <span class="o">=&gt;</span> <span class="p">{</span>
    <span class="kd">let</span> <span class="nx">xy</span> <span class="o">=</span> <span class="nx">x</span> <span class="o">*</span> <span class="nx">y</span><span class="p">;</span>
    <span class="kd">let</span> <span class="nx">tuple</span> <span class="o">=</span> <span class="p">(</span><span class="nx">xy</span><span class="p">,</span> <span class="nx">x</span><span class="p">);</span>
    <span class="kd">let</span> <span class="nx">f</span> <span class="o">=</span> <span class="nf">first</span><span class="p">(</span><span class="nx">tuple</span><span class="p">);</span>
    <span class="kd">let</span> <span class="nx">s</span> <span class="o">=</span> <span class="nf">second</span><span class="p">(</span><span class="nx">tuple</span><span class="p">);</span>
    <span class="nx">f</span> <span class="o">*</span> <span class="nx">s</span>
  <span class="p">};</span>

  <span class="nf">iter</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="nx">work_closure</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="p">};</span>

<span class="kd">let</span> <span class="nx">iteration</span> <span class="o">=</span> <span class="nf">iter</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="nx">work</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>

<span class="nf">print</span><span class="p">(</span><span class="nx">iteration</span><span class="p">)</span>
</pre></table></code></div></div><p>This program does not do anything useful, but it exercises a few things:</p><ul><li>Tuples (always 2 elements, fetch them using built-in functions <code class="language-plaintext highlighter-rouge">first</code> and <code class="language-plaintext highlighter-rouge">second</code>)<li>Closures<li>High-order functions (take functions by parameter and execute them)<li>Tail-calls and Tail-call optimization<li>Some binary operators</ul><p>This was my benchmarking script for the rest of the competition. Turns out the final tests didn’t go that route, but I wanted this thing to run faster and faster.</p><p>Just for curiosity, you can build lists in Rinha by using tuples: since tuples can have tuples inside them, you could have something like <code class="language-plaintext highlighter-rouge">(1, (2, (3, (4, "&lt;nil&gt;"))))</code> or something like that. You can also have maps using a clever closure trick:</p><div class="language-js highlighter-rouge"><div class="code-header"> <span label-text="JavaScript"><i class="fas fa-code small"></i></span> <button aria-label="copy" title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
</pre><td class="rouge-code"><pre><span class="kd">let</span> <span class="nx">empty_map</span> <span class="o">=</span> <span class="nf">fn</span><span class="p">(</span><span class="nx">v</span><span class="p">)</span> <span class="o">=&gt;</span> <span class="p">{</span>
    <span class="p">(</span><span class="dl">"</span><span class="s2">&lt;KeyNotFound&gt;</span><span class="dl">"</span><span class="p">,</span> <span class="dl">"</span><span class="s2">undefined key </span><span class="dl">"</span> <span class="o">+</span> <span class="nx">v</span><span class="p">)</span>
<span class="p">};</span>

<span class="kd">let</span> <span class="kd">set</span> <span class="o">=</span> <span class="nf">fn</span><span class="p">(</span><span class="kd">var</span><span class="p">,</span> <span class="nx">val</span><span class="p">,</span> <span class="nx">m</span><span class="p">)</span> <span class="o">=&gt;</span> <span class="p">{</span>
    <span class="nf">fn</span><span class="p">(</span><span class="nx">v</span><span class="p">)</span> <span class="o">=&gt;</span> <span class="p">{</span>
        <span class="k">if </span><span class="p">(</span><span class="kd">var</span> <span class="o">==</span> <span class="nx">v</span><span class="p">)</span> <span class="p">{</span>
            <span class="nx">val</span>
        <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
            <span class="nf">m</span><span class="p">(</span><span class="nx">v</span><span class="p">)</span>
        <span class="p">}</span>
    <span class="p">}</span>
<span class="p">};</span>

<span class="kd">let</span> <span class="nx">map</span> <span class="o">=</span> <span class="nf">set</span><span class="p">(</span><span class="dl">"</span><span class="s2">z</span><span class="dl">"</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="nx">empty_map</span><span class="p">);</span>
<span class="kd">let</span> <span class="nx">map</span> <span class="o">=</span> <span class="nf">set</span><span class="p">(</span><span class="dl">"</span><span class="s2">x</span><span class="dl">"</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="nx">map</span><span class="p">);</span>

<span class="kd">let</span> <span class="nx">v</span> <span class="o">=</span> <span class="nf">map</span><span class="p">(</span><span class="dl">"</span><span class="s2">z</span><span class="dl">"</span><span class="p">);</span>
<span class="nf">print</span><span class="p">(</span><span class="nx">v</span><span class="p">)</span>
</pre></table></code></div></div><p>The inner function <code class="language-plaintext highlighter-rouge">fn(v)</code> inside <code class="language-plaintext highlighter-rouge">set</code> takes the current map (represented by the function <code class="language-plaintext highlighter-rouge">m</code>) by closure. In fact, all <code class="language-plaintext highlighter-rouge">set</code> parameters are captured by the inner function when <code class="language-plaintext highlighter-rouge">set</code> is executed, so you can use closures as a mechanism to hold state. To fetch a value, we just recursively call <code class="language-plaintext highlighter-rouge">m</code> until we find a closure whose <code class="language-plaintext highlighter-rouge">var</code> is our target. It’s a slow <code class="language-plaintext highlighter-rouge">O(n)</code> map, for sure, but a map nonetheless.</p><p>It’s interesting to see how powerful functional programming can be: this language does not have built-in types for lists or maps, and yet we can represent those concepts, albeit not in the most performant way. Nonetheless, this idea was used by one of the competitors to make a <code class="language-plaintext highlighter-rouge">meta rinha</code> program, which is an interpreter built in the Rinha language itself. In fact, the example above comes almost straight from the meta rinha program, I just changed the names. I used meta rinha to find bugs with closures in my interpreter. I think an interpreter that runs another interpreter is a good quick measure for correctness, everything else in the interpreter is simpler than closure handling. This is an example of a meta-rinha program:</p><div class="language-js highlighter-rouge"><div class="code-header"> <span label-text="JavaScript"><i class="fas fa-code small"></i></span> <button aria-label="copy" title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
</pre><td class="rouge-code"><pre><span class="kd">let</span> <span class="nx">fac</span> <span class="o">=</span>
  <span class="p">(</span><span class="nx">ExpLet</span><span class="p">,</span> <span class="p">(</span><span class="dl">"</span><span class="s2">f</span><span class="dl">"</span><span class="p">,</span> <span class="p">(</span>
    <span class="p">(</span><span class="nx">ExpLambda</span><span class="p">,</span> <span class="p">(</span><span class="dl">"</span><span class="s2">x</span><span class="dl">"</span><span class="p">,</span>
       <span class="p">(</span><span class="nx">ExpIf</span><span class="p">,</span> <span class="p">((</span><span class="nx">ExpVar</span><span class="p">,</span> <span class="dl">"</span><span class="s2">x</span><span class="dl">"</span><span class="p">),</span>
              <span class="p">((</span><span class="nx">ExpMul</span><span class="p">,</span> <span class="p">((</span><span class="nx">ExpVar</span><span class="p">,</span> <span class="dl">"</span><span class="s2">x</span><span class="dl">"</span><span class="p">),</span>
                      <span class="p">(</span><span class="nx">ExpApp</span><span class="p">,</span> <span class="p">((</span><span class="nx">ExpVar</span><span class="p">,</span> <span class="dl">"</span><span class="s2">f</span><span class="dl">"</span><span class="p">),</span> <span class="p">(</span><span class="nx">ExpSub</span><span class="p">,</span> <span class="p">((</span><span class="nx">ExpVar</span><span class="p">,</span> <span class="dl">"</span><span class="s2">x</span><span class="dl">"</span><span class="p">),</span> <span class="p">(</span><span class="nx">ExpK</span><span class="p">,</span> <span class="mi">1</span><span class="p">))))))),</span>
              <span class="p">(</span><span class="nx">ExpK</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))))),</span>
    <span class="p">(</span><span class="nx">ExpApp</span><span class="p">,</span> <span class="p">((</span><span class="nx">ExpVar</span><span class="p">,</span> <span class="dl">"</span><span class="s2">f</span><span class="dl">"</span><span class="p">),</span> <span class="p">(</span><span class="nx">ExpK</span><span class="p">,</span> <span class="mi">10</span><span class="p">))))));</span>


<span class="kd">let</span> <span class="nx">_</span> <span class="o">=</span> <span class="nf">print</span><span class="p">(</span><span class="dl">"</span><span class="s2">factorial 10: </span><span class="dl">"</span> <span class="o">+</span> <span class="nf">second</span><span class="p">(</span><span class="nf">eval</span><span class="p">(</span><span class="nx">fac</span><span class="p">,</span> <span class="nx">empty_env</span><span class="p">)));</span>
</pre></table></code></div></div><h1 id="my-entry">My entry</h1><p>My idea for Rinha was to build an initial simple reasonably optimized interpreter and then try to type-check it for AOT, but that didn’t work. I did learn a bit about Hindley Milner type inference, but the input language is just too dynamic, there are no type annotations, and it would need some Haskell-like type classes to make operator overloading work (i.e. the + does integer sum and string concat). That’s too complex for the timeframe I had, so I decided to stick with an interpreter. I wouldn’t have much time to dedicate to the Rinha before the competition as I was traveling, but I had been curious about an article I had read about a tree-walker-like interpreter, but on steroids. I did experiment with the approach in the past on a smaller scale, but I thought it was finally time to make it for real</p><p>Some time ago, I read a <a href="https://blog.cloudflare.com/building-fast-interpreters-in-rust/">Cloudflare article</a> in which they built an interpreter for Wireshark filters to add it to their firewall solutions. Their first solution was a treewalker, which was alright… it would be nice if they were a bit faster though. However, the risks and overheads of JITting the filters to all platforms are way too high, outweighing any benefits. They are processing untrusted network input, and doing low-level unsafe programming (like running JIT, generating machine code, marking sections of memory as executable, and so on) also didn’t seem like a good idea at the scale they deal with.</p><p>Instead, they did a HOAS approach (according to Sofia). HOAS means High-Order Abstract Syntax, and I will not pretend I know what it means. My entry uses Rust, and I actually call it Lambda compilation, because it’s all lambdas. It’s all dynamic dispatch and closures. HOAS means you use the host language’s features to run the language, but it’s only true HOAS for the most simple of languages, like lambda calculus. For anything a bit more complicated it’s more difficult to pull it off in a pure HOAS fashion. ChatGPT tells me it’s actually a mixture of First-Order Abstract Syntax (FOAS) and HOAS, but I will also not pretend I know what exactly it means. Therefore, let’s go with lambda compilation, it describes exactly what happens in the code.</p><p>Also, bear in mind that I won’t worry too much about memory leaks and memory consumption in general, this is a toy interpreter for a competition, not production usage. Still, I think some things I learned would work well in a production interpreter, while for others I exploited the nature of the competition and just did some hacky stuff to squeeze performance.</p><p>For the lambda compilation approach, suppose we need to compile a binary expression that loads a constant value and a variable, I would need this code:</p><div class="language-rust highlighter-rouge"><div class="code-header"> <span label-text="Rust"><i class="fas fa-code small"></i></span> <button aria-label="copy" title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
</pre><td class="rouge-code"><pre>
<span class="k">pub</span> <span class="k">type</span> <span class="n">LambdaFunction</span> <span class="o">=</span> <span class="nb">Box</span><span class="o">&lt;</span><span class="k">dyn</span> <span class="nf">Fn</span><span class="p">(</span><span class="o">&amp;</span><span class="k">mut</span> <span class="n">ExecutionContext</span><span class="p">)</span> <span class="k">-&gt;</span> <span class="n">Value</span><span class="o">&gt;</span><span class="p">;</span>

<span class="c1">// Enum for runtime values</span>
<span class="k">pub</span> <span class="k">enum</span> <span class="n">Value</span> <span class="p">{</span>
    <span class="nf">Int</span><span class="p">(</span><span class="nb">i32</span><span class="p">),</span>
    <span class="nf">Bool</span><span class="p">(</span><span class="nb">bool</span><span class="p">),</span>
    <span class="nf">Str</span><span class="p">(</span><span class="nb">String</span><span class="p">),</span>
    <span class="nf">Tuple</span><span class="p">(</span><span class="nb">Box</span><span class="o">&lt;</span><span class="n">Value</span><span class="o">&gt;</span><span class="p">,</span> <span class="nb">Box</span><span class="o">&lt;</span><span class="n">Value</span><span class="o">&gt;</span><span class="p">),</span>
    <span class="nf">Closure</span><span class="p">(</span><span class="o">...</span><span class="p">),</span>
    <span class="c1">//and so on</span>
<span class="p">}</span>
<span class="o">...</span>

<span class="k">fn</span> <span class="nf">compile</span><span class="p">(</span><span class="o">&amp;</span><span class="k">self</span><span class="p">,</span> <span class="n">ast</span><span class="p">:</span> <span class="o">&amp;</span><span class="n">Term</span><span class="p">)</span> <span class="k">-&gt;</span> <span class="n">LambdaFunction</span> <span class="p">{</span>
  <span class="k">match</span> <span class="n">ast</span> <span class="p">{</span>
    <span class="nn">Term</span><span class="p">::</span><span class="n">Int</span> <span class="p">{</span> <span class="n">value</span> <span class="p">}</span> <span class="k">=&gt;</span> <span class="nn">Box</span><span class="p">::</span><span class="nf">new</span><span class="p">(</span><span class="k">move</span> <span class="p">|</span><span class="n">exec_context</span><span class="p">|</span> <span class="nn">Value</span><span class="p">::</span><span class="nf">Int</span><span class="p">(</span><span class="n">value</span><span class="p">)),</span>
    <span class="nn">Term</span><span class="p">::</span><span class="n">Var</span> <span class="p">{</span> <span class="n">name</span><span class="p">,</span> <span class="o">..</span> <span class="p">}</span> <span class="k">=&gt;</span> <span class="nn">Box</span><span class="p">::</span><span class="nf">new</span><span class="p">(</span><span class="k">move</span> <span class="p">|</span><span class="n">exec_context</span><span class="p">|</span> <span class="n">exec_context</span><span class="nf">.cur_frame</span><span class="p">()</span><span class="py">.variables</span><span class="nf">.get</span><span class="p">(</span><span class="o">&amp;</span><span class="n">name</span><span class="p">)</span><span class="nf">.unwrap</span><span class="p">()</span><span class="nf">.clone</span><span class="p">())</span>
    <span class="nn">Term</span><span class="p">::</span><span class="n">BinaryExp</span> <span class="p">{</span> <span class="n">op</span><span class="p">,</span> <span class="n">lhs</span><span class="p">,</span> <span class="n">rhs</span> <span class="p">}</span> <span class="k">=&gt;</span> <span class="p">{</span>
      <span class="k">let</span> <span class="n">lhs_compiled</span> <span class="o">=</span> <span class="k">self</span><span class="nf">.compile</span><span class="p">(</span><span class="n">lhs</span><span class="p">);</span>
      <span class="k">let</span> <span class="n">rhs_compiled</span> <span class="o">=</span> <span class="k">self</span><span class="nf">.compile</span><span class="p">(</span><span class="n">lhs</span><span class="p">);</span>
      <span class="k">match</span> <span class="n">op</span> <span class="p">{</span>
        <span class="nn">Operator</span><span class="p">::</span><span class="n">Plus</span> <span class="k">=&gt;</span>
          <span class="nn">Box</span><span class="p">::</span><span class="nf">new</span><span class="p">(</span><span class="k">move</span> <span class="p">|</span><span class="n">ec</span><span class="p">|</span> <span class="p">{</span>
            <span class="k">let</span> <span class="n">lhs</span> <span class="o">=</span> <span class="nf">lhs_compiled</span><span class="p">(</span><span class="n">ec</span><span class="p">);</span>
            <span class="k">let</span> <span class="n">rhs</span> <span class="o">=</span> <span class="nf">rhs_compiled</span><span class="p">(</span><span class="n">ec</span><span class="p">);</span>
            <span class="k">match</span> <span class="p">(</span><span class="n">lhs</span><span class="p">,</span> <span class="n">rhs</span><span class="p">)</span> <span class="k">=&gt;</span> <span class="p">{</span>
              <span class="nn">Value</span><span class="p">::</span><span class="nf">Int</span><span class="p">(</span><span class="n">l</span><span class="p">),</span> <span class="nn">Value</span><span class="p">::</span><span class="nf">Int</span><span class="p">(</span><span class="n">r</span><span class="p">)</span> <span class="k">=&gt;</span> <span class="nn">Value</span><span class="p">::</span><span class="nf">Int</span><span class="p">(</span><span class="n">l</span> <span class="o">+</span> <span class="n">r</span><span class="p">),</span>
              <span class="c1">//... also handles string and int concat</span>
            <span class="p">}</span>
          <span class="p">}),</span>
        <span class="c1">//... also handles other operators</span>
      <span class="p">}</span>
    <span class="p">}</span>
    <span class="o">...</span>
  <span class="p">}</span>
<span class="p">}</span>

</pre></table></code></div></div><p>Instead of matching the AST at runtime, we try to pre-compile every decision that the tree walker would make. For instance, a regular treewalker would have no choice but to pattern-match every kind of operator (plus, multiply, minus, etc) at every execution of a binary operation. In our scenario, we already looked into it and determined we only need to do the dynamic type checking to ensure both sides are <code class="language-plaintext highlighter-rouge">Int</code>, and the operator + handler is returned directly.</p><p>How much faster is this compared to a regular tree walker? Cloudflare claims it’s 10~15%, and their article also uses Rust, but the language is very different.</p><p>The problem is: I didn’t measure it in Rust. However, I did measure it in Ocaml using <code class="language-plaintext highlighter-rouge">ocamlopt</code>, and I have this <a href="https://github.com/ricardopieper/rinha-ocaml">OCaml version of the Rinha interpreter</a> that compares the two approaches. In general, I’m getting around 8% improvement using this approach. Doing it in OCaml was much more of a joy compared to Rust, but doing it in Rust wasn’t too difficult either. I’d say this lambda compilation strategy is a bit more complicated to debug though.</p><p>So I wrote the initial implementation and tried to run the perf.rinha file, here it is in case you don’t want to scroll all the way back:</p><div class="language-js highlighter-rouge"><div class="code-header"> <span label-text="JavaScript"><i class="fas fa-code small"></i></span> <button aria-label="copy" title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
</pre><td class="rouge-code"><pre><span class="kd">let</span> <span class="nx">iter</span> <span class="o">=</span> <span class="nf">fn </span><span class="p">(</span><span class="k">from</span><span class="p">,</span> <span class="nx">to</span><span class="p">,</span> <span class="nx">call</span><span class="p">,</span> <span class="nx">prev</span><span class="p">)</span> <span class="o">=&gt;</span> <span class="p">{</span>
  <span class="k">if </span><span class="p">(</span><span class="k">from</span> <span class="o">&lt;</span> <span class="nx">to</span><span class="p">)</span> <span class="p">{</span>
    <span class="kd">let</span> <span class="nx">res</span> <span class="o">=</span> <span class="nf">call</span><span class="p">(</span><span class="k">from</span><span class="p">);</span>
    <span class="nf">iter</span><span class="p">(</span><span class="k">from</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="nx">to</span><span class="p">,</span> <span class="nx">call</span><span class="p">,</span> <span class="nx">res</span><span class="p">)</span>
  <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
    <span class="nx">prev</span>
  <span class="p">}</span>
<span class="p">};</span>

<span class="kd">let</span> <span class="nx">work</span> <span class="o">=</span> <span class="nf">fn</span><span class="p">(</span><span class="nx">x</span><span class="p">)</span> <span class="o">=&gt;</span> <span class="p">{</span>
  <span class="kd">let</span> <span class="nx">work_closure</span> <span class="o">=</span> <span class="nf">fn</span><span class="p">(</span><span class="nx">y</span><span class="p">)</span> <span class="o">=&gt;</span> <span class="p">{</span>
    <span class="kd">let</span> <span class="nx">xy</span> <span class="o">=</span> <span class="nx">x</span> <span class="o">*</span> <span class="nx">y</span><span class="p">;</span>
    <span class="kd">let</span> <span class="nx">tuple</span> <span class="o">=</span> <span class="p">(</span><span class="nx">xy</span><span class="p">,</span> <span class="nx">x</span><span class="p">);</span>
    <span class="kd">let</span> <span class="nx">f</span> <span class="o">=</span> <span class="nf">first</span><span class="p">(</span><span class="nx">tuple</span><span class="p">);</span>
    <span class="kd">let</span> <span class="nx">s</span> <span class="o">=</span> <span class="nf">second</span><span class="p">(</span><span class="nx">tuple</span><span class="p">);</span>
    <span class="nx">f</span> <span class="o">*</span> <span class="nx">s</span>
  <span class="p">};</span>

  <span class="nf">iter</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="nx">work_closure</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="p">};</span>

<span class="kd">let</span> <span class="nx">iteration</span> <span class="o">=</span> <span class="nf">iter</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="nx">work</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>

<span class="nf">print</span><span class="p">(</span><span class="nx">iteration</span><span class="p">)</span>
</pre></table></code></div></div><p>This code ran in 43ms. That’s our baseline. It’s pretty bad. Let’s optimize it.</p><h2 id="optimization-1-btreemap-instead-of-hashmap">Optimization 1: BTreeMap instead of HashMap<a href="#optimization-1-btreemap-instead-of-hashmap"><i class="fas fa-hashtag"></i></a></h2></h2><p>In the interpreter VM state, I store variables in the call frame data. Each time a function is called, I create a new call frame with an empty <code class="language-plaintext highlighter-rouge">HashMap&lt;&amp;str, Value&gt;</code> map for the variables, and store all let bindings, closure, and params there.</p><p>This first optimization is just changing that map to a <code class="language-plaintext highlighter-rouge">BTreeMap&lt;&amp;str, Value&gt;</code> instead. Hashing strings is kinda expensive, so let’s stop doing that.</p><p>After this change, we’re down to 28ms, a 49% improvement.</p><h2 id="optimization-2-use-vec-instead-of-any-kind-of-map">Optimization 2: Use Vec instead of any kind of map<a href="#optimization-2-use-vec-instead-of-any-kind-of-map"><i class="fas fa-hashtag"></i></a></h2></h2><p>During compilation, we scan all variables in the program, and I actually have a process in which I intern their strings. I end up with a list of all strings used for variables in the program, like <code class="language-plaintext highlighter-rouge">[iter, from, to, call, prev, res, work, work_closure, y, xy, tuple, f, s, iteration]</code>, 14 variables. Instead of strings, I have a <code class="language-plaintext highlighter-rouge">usize</code> number for each one, where <code class="language-plaintext highlighter-rouge">iter=0, from=1, </code> and so on. So this is how it worked:</p><ul><li>At compile time, I tell the call_function procedure to create a call stack with 14 slots<li>Copy parameters and closures into these 14 slots, as available<li>Whenever I need to fetch these values, I just tell which slot index I want. At compile time this becomes a simple <code class="language-plaintext highlighter-rouge">stack_frame.variables[pos]</code>, no map lookup or hashing needed<li>????<li>Profit</ul><p>The result was… underwhelming. Now we’re slower than before, 35ms. This is because each new frame eventually needs an allocation of size 14, and the size of <code class="language-plaintext highlighter-rouge">Value</code> at this point I think was around 24 bytes. This is a bit of foreshadowing so let’s keep going.</p><h2 id="optimization-3-use-a-separate-vec-for-each-variable-and-track-their-evolution-separately">Optimization 3: Use a separate Vec for each variable and track their evolution separately<a href="#optimization-3-use-a-separate-vec-for-each-variable-and-track-their-evolution-separately"><i class="fas fa-hashtag"></i></a></h2></h2><p>To avoid every new frame needing a new big allocation, we need to make this process trivial. Therefore, I started tracking the variables in my global VM state:</p><div class="language-rust highlighter-rouge"><div class="code-header"> <span label-text="Rust"><i class="fas fa-code small"></i></span> <button aria-label="copy" title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre><td class="rouge-code"><pre><span class="k">struct</span> <span class="n">ExecutionContext</span> <span class="p">{</span>
  <span class="o">...</span>
  <span class="n">call_stack</span><span class="p">:</span> <span class="nb">Vec</span><span class="o">&lt;</span><span class="n">CallFrame</span><span class="o">&gt;</span><span class="p">,</span>
  <span class="n">variables</span><span class="p">:</span> <span class="nb">Vec</span><span class="o">&lt;</span><span class="nb">Vec</span><span class="o">&lt;</span><span class="n">Value</span><span class="o">&gt;&gt;</span>
  <span class="o">...</span>
<span class="p">}</span>

</pre></table></code></div></div><p>This way, <code class="language-plaintext highlighter-rouge">variables</code> became a 2D vector in which the first dimension identifies the variable, and the second dimension is just the variable’s “stack”. So instead of tracking frames of variables, I track…. variables of frames? That’s confusing, but I hope it makes some sense. Reminds me of column databases, where a table is not millions of rows broken apart into many cells for each field: The table has one big array for each of its fields, and each index into that field identifies a record.</p><p>To access the variable <code class="language-plaintext highlighter-rouge">from</code>, one just needs to to <code class="language-plaintext highlighter-rouge">self.variables[0].last().unwrap()</code>, and new call frames just need to push new values onto the respective indices for each variable. When a stack frame ends, we need to pop the values that were used, therefore, the call frame tracks which variables were pushed:</p><div class="language-rust highlighter-rouge"><div class="code-header"> <span label-text="Rust"><i class="fas fa-code small"></i></span> <button aria-label="copy" title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre><td class="rouge-code"><pre><span class="k">struct</span> <span class="n">CallFrame</span> <span class="p">{</span>
 <span class="o">...</span>
 <span class="n">pushed_vars</span><span class="p">:</span> <span class="nb">Vec</span><span class="o">&lt;</span><span class="nb">usize</span><span class="o">&gt;</span>

<span class="p">}</span>
</pre></table></code></div></div><p>When we pop a frame, we read the pushed_vars and pop each variable:</p><div class="language-rust highlighter-rouge"><div class="code-header"> <span label-text="Rust"><i class="fas fa-code small"></i></span> <button aria-label="copy" title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre><td class="rouge-code"><pre><span class="k">fn</span> <span class="nf">pop_frame</span><span class="p">(</span><span class="o">&amp;</span><span class="k">self</span><span class="p">)</span> <span class="p">{</span>
  <span class="k">let</span> <span class="n">frame</span> <span class="o">=</span> <span class="k">self</span><span class="py">.call_stack</span><span class="nf">.pop</span><span class="p">()</span><span class="nf">.unwrap</span><span class="p">();</span>
  <span class="k">for</span> <span class="n">v</span> <span class="k">in</span> <span class="n">frame</span><span class="py">.pushed_vars</span> <span class="p">{</span>
    <span class="k">self</span><span class="py">.variables</span><span class="p">[</span><span class="n">v</span><span class="p">]</span><span class="nf">.pop</span><span class="p">()</span>
  <span class="p">}</span>
<span class="p">}</span>

</pre></table></code></div></div><p>That optimization brings us to 14ms, the fastest yet. That’s a 204% improvement from baseline.</p><h2 id="optimization-4-reuse-frame-allocations">Optimization 4: Reuse frame allocations<a href="#optimization-4-reuse-frame-allocations"><i class="fas fa-hashtag"></i></a></h2></h2><p>Let’s take a look at our fib function:</p><div class="language-js highlighter-rouge"><div class="code-header"> <span label-text="JavaScript"><i class="fas fa-code small"></i></span> <button aria-label="copy" title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
</pre><td class="rouge-code"><pre><span class="kd">let</span> <span class="nx">fib</span> <span class="o">=</span> <span class="nf">fn </span><span class="p">(</span><span class="nx">n</span><span class="p">)</span> <span class="o">=&gt;</span> <span class="p">{</span>
  <span class="k">if </span><span class="p">(</span><span class="nx">n</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">)</span> <span class="p">{</span>
    <span class="nx">n</span>
  <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
    <span class="nf">fib</span><span class="p">(</span><span class="nx">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="nf">fib</span><span class="p">(</span><span class="nx">n</span> <span class="o">-</span> <span class="mi">2</span><span class="p">)</span>
  <span class="p">}</span>
<span class="p">};</span>

<span class="nf">print</span><span class="p">(</span><span class="dl">"</span><span class="s2">fib: </span><span class="dl">"</span> <span class="o">+</span> <span class="nf">fib</span><span class="p">(</span><span class="mi">10</span><span class="p">))</span>
</pre></table></code></div></div><p>This code does a lot of unnecessary work, allocates a lot of call frames, it’s incredibly wasteful. To keep track of pushed vars, we are still doing a lot of allocations and array resizing. What if we could reuse these allocated spaces?</p><p>In the line <code class="language-plaintext highlighter-rouge">fib(n - 1) + fib(n - 2)</code>, we will execute <code class="language-plaintext highlighter-rouge">fib(n-1)</code> which will allocate some frame data, and then dispose of the frame. But we could instead store that frame into a <code class="language-plaintext highlighter-rouge">reusable_frames</code> array, and at every new frame, we pop this array and clear the data inside the pushed_vars. This does not free the allocation, allowing us to reuse it. This helps on the <code class="language-plaintext highlighter-rouge">perf.rinha</code> benchmark too, as we execute the functions in a loop, their frames are also reused.</p><p>This resulted in a 91% improvement from our last optimization, down to 7.4ms. We’re at a 482% improvement from baseline.</p><h2 id="optimization-5-non-heap-tuples">Optimization 5: Non-heap tuples<a href="#optimization-5-non-heap-tuples"><i class="fas fa-hashtag"></i></a></h2></h2><p>Let’s take a look again at our Value enum:</p><div class="language-rust highlighter-rouge"><div class="code-header"> <span label-text="Rust"><i class="fas fa-code small"></i></span> <button aria-label="copy" title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
</pre><td class="rouge-code"><pre><span class="k">pub</span> <span class="k">enum</span> <span class="n">Value</span> <span class="p">{</span>
  <span class="nf">Int</span><span class="p">(</span><span class="nb">i32</span><span class="p">),</span>
  <span class="nf">Bool</span><span class="p">(</span><span class="nb">bool</span><span class="p">),</span>
  <span class="nf">Str</span><span class="p">(</span><span class="nb">String</span><span class="p">),</span>
  <span class="nf">Tuple</span><span class="p">(</span><span class="nb">Box</span><span class="o">&lt;</span><span class="n">Value</span><span class="o">&gt;</span><span class="p">,</span> <span class="nb">Box</span><span class="o">&lt;</span><span class="n">Value</span><span class="o">&gt;</span><span class="p">),</span>
  <span class="nf">Closure</span><span class="p">(</span><span class="o">...</span><span class="p">),</span>
  <span class="c1">//and so on</span>
<span class="p">}</span>
</pre></table></code></div></div><p>Our <code class="language-plaintext highlighter-rouge">perf.rinha</code> creates a tuple, which results in 2 heap allocations. <code class="language-plaintext highlighter-rouge">Value</code> can be of any variant, and since using them directly would cause infinite recursion, they must be <code class="language-plaintext highlighter-rouge">Box&lt;Value&gt;</code> so I can have things like <code class="language-plaintext highlighter-rouge">(1, (true, ("foo", fn() {})))</code>.</p><p><strong>Or do we 🤨?</strong> (cue in Vsauce music)</p><p>Good news: <code class="language-plaintext highlighter-rouge">perf.rinha</code> allocates only <code class="language-plaintext highlighter-rouge">(i32, i32)</code> tuples. So here’s the idea:</p><div class="language-rust highlighter-rouge"><div class="code-header"> <span label-text="Rust"><i class="fas fa-code small"></i></span> <button aria-label="copy" title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
</pre><td class="rouge-code"><pre><span class="k">pub</span> <span class="k">enum</span> <span class="n">Value</span> <span class="p">{</span>
  <span class="nf">Int</span><span class="p">(</span><span class="nb">i32</span><span class="p">),</span>
  <span class="nf">Bool</span><span class="p">(</span><span class="nb">bool</span><span class="p">),</span>
  <span class="nf">Str</span><span class="p">(</span><span class="nb">String</span><span class="p">),</span>
  <span class="nf">IntTuple</span><span class="p">(</span><span class="nb">i32</span><span class="p">,</span> <span class="nb">i32</span><span class="p">),</span>
  <span class="nf">BoolIntTuple</span><span class="p">(</span><span class="nb">bool</span><span class="p">,</span> <span class="nb">i32</span><span class="p">),</span>
  <span class="nf">IntBoolTuple</span><span class="p">(</span><span class="nb">i32</span><span class="p">,</span> <span class="nb">bool</span><span class="p">),</span>
  <span class="nf">BoolTuple</span><span class="p">(</span><span class="nb">bool</span><span class="p">,</span> <span class="nb">bool</span><span class="p">),</span>
  <span class="nf">Tuple</span><span class="p">(</span><span class="nb">Box</span><span class="o">&lt;</span><span class="n">Value</span><span class="o">&gt;</span><span class="p">,</span> <span class="nb">Box</span><span class="o">&lt;</span><span class="n">Value</span><span class="o">&gt;</span><span class="p">),</span>
  <span class="o">...</span>
<span class="p">}</span>
</pre></table></code></div></div><p>By creating these extra primitive-only tuples, we avoid talking with the allocator. This resulted in a 118% improvement in runtime, down to 3.4ms. We were at 7.4ms before. This is ~1170% improvement from baseline.</p><p>However, this is where the low-hanging fruit ends. My final result will be 1.55ms, and the remaining 1.9ms will be difficult to shave off. <strong>But let’s take a detour for a moment</strong>, because it’s time to improve our support for recursive functions.</p><h2 id="optimization-6-trampolines">Optimization 6: Trampolines<a href="#optimization-6-trampolines"><i class="fas fa-hashtag"></i></a></h2></h2><p>This item will explain the implementation of Tail-Call optimization, but we won’t gain much performance yet. At this point in the competition, I was worried about recursion. Let’s take a look at the classical recursive Fibonacci algorithm again:</p><div class="language-js highlighter-rouge"><div class="code-header"> <span label-text="JavaScript"><i class="fas fa-code small"></i></span> <button aria-label="copy" title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
</pre><td class="rouge-code"><pre><span class="kd">let</span> <span class="nx">fib</span> <span class="o">=</span> <span class="nf">fn </span><span class="p">(</span><span class="nx">n</span><span class="p">)</span> <span class="o">=&gt;</span> <span class="p">{</span>
  <span class="k">if </span><span class="p">(</span><span class="nx">n</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">)</span> <span class="p">{</span>
    <span class="nx">n</span>
  <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
    <span class="nf">fib</span><span class="p">(</span><span class="nx">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="nf">fib</span><span class="p">(</span><span class="nx">n</span> <span class="o">-</span> <span class="mi">2</span><span class="p">)</span>
  <span class="p">}</span>
<span class="p">};</span>

<span class="nf">print</span><span class="p">(</span><span class="dl">"</span><span class="s2">fib: </span><span class="dl">"</span> <span class="o">+</span> <span class="nf">fib</span><span class="p">(</span><span class="mi">10</span><span class="p">))</span>
</pre></table></code></div></div><p>If you translate this code to Rust or C and compile it using optimizations, it transforms this code into a loop. There is an optimization step in LLVM that transforms algorithms such as fib into their iterative form. It only does this sometimes, not always, because some constraints need to hold.</p><p>This Fibonacci algorithm, as is, is not in tail call form. There’s nothing I can do to make it a tail call. I could look into what LLVM does but that would be too much for the time I had.</p><p>However, in functional programming, you can just write your function in tail call form:</p><div class="language-js highlighter-rouge"><div class="code-header"> <span label-text="JavaScript"><i class="fas fa-code small"></i></span> <button aria-label="copy" title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre><td class="rouge-code"><pre><span class="kd">let</span> <span class="nx">fib_tc</span> <span class="o">=</span> <span class="nf">fn </span><span class="p">(</span><span class="nx">n</span><span class="p">,</span> <span class="nx">a</span><span class="p">,</span> <span class="nx">b</span><span class="p">)</span> <span class="o">=&gt;</span> <span class="p">{</span>
  <span class="k">if </span><span class="p">(</span><span class="nx">n</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
    <span class="nx">a</span>
  <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
    <span class="nf">fib_tc</span><span class="p">(</span><span class="nx">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="nx">b</span><span class="p">,</span> <span class="nx">a</span> <span class="o">+</span> <span class="nx">b</span><span class="p">)</span>
  <span class="p">}</span>
<span class="p">};</span>
</pre></table></code></div></div><p>Whenever the last thing a function does is a call to itself, we can actually reuse the stack frame by transforming it into this format:</p><div class="language-js highlighter-rouge"><div class="code-header"> <span label-text="JavaScript"><i class="fas fa-code small"></i></span> <button aria-label="copy" title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre><td class="rouge-code"><pre><span class="kd">let</span> <span class="nx">fib_tc</span> <span class="o">=</span> <span class="nf">fn </span><span class="p">(</span><span class="nx">n</span><span class="p">,</span> <span class="nx">a</span><span class="p">,</span> <span class="nx">b</span><span class="p">)</span> <span class="o">=&gt;</span> <span class="p">{</span>
  <span class="k">if </span><span class="p">(</span><span class="nx">n</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
    <span class="nx">a</span>
  <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
    <span class="nf">fn </span><span class="p">()</span> <span class="o">=&gt;</span> <span class="nf">fib_tc</span><span class="p">(</span><span class="nx">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="nx">b</span><span class="p">,</span> <span class="nx">a</span> <span class="o">+</span> <span class="nx">b</span><span class="p">)</span> <span class="c1">//This became a function declaration</span>
  <span class="p">}</span>
<span class="p">};</span>
</pre></table></code></div></div><p>In this case, the compiler can apply a pre-processing step on the AST and wrap the function call in another function instead of evaluating it immediately.</p><p>Then at runtime, the interpreter does something like this, in pseudocode:</p><div class="language-js highlighter-rouge"><div class="code-header"> <span label-text="JavaScript"><i class="fas fa-code small"></i></span> <button aria-label="copy" title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre><td class="rouge-code"><pre><span class="kd">let</span> <span class="nx">result</span> <span class="o">=</span> <span class="nf">call</span><span class="p">(</span><span class="nx">fib_tc</span><span class="p">,</span> <span class="p">...);</span>

<span class="k">while </span><span class="p">(</span><span class="nf">is_callable</span><span class="p">(</span><span class="nx">result</span><span class="p">))</span> <span class="p">{</span>
  <span class="nx">result</span> <span class="o">=</span> <span class="nf">result</span><span class="p">();</span>
<span class="p">}</span>

<span class="k">return</span> <span class="nx">result</span>
</pre></table></code></div></div><p>Notice that when we call fib_tc, we only create one stack frame. This stack frame is popped when the call ends, and then the <code class="language-plaintext highlighter-rouge">while</code> loop checks if the result is a callable value. If so, we call it in a new stack frame, but we popped the stack before so it won’t create a deep stack. The function returns (stack frame is popped), and so on. Due to this jumping behavior up and down in the stack, this technique is called a <code class="language-plaintext highlighter-rouge">trampoline</code>.</p><p>This technique avoids the creation of a deep stack that potentially overflows. There is also CPS (Continuation-Passing Style) that can even handle the original recursive <code class="language-plaintext highlighter-rouge">fib</code> function, but they are more complex to implement. The trampoline technique will suffice for now.</p><p>This resulted in no performance gain, but it’s a nice feature… fear not, the trampoline will come in clutch in a few paragraphs.</p><h2 id="optimization-7-reduce-the-size-of-the-value">Optimization 7: Reduce the size of the <code class="language-plaintext highlighter-rouge">Value</code><a href="#optimization-7-reduce-the-size-of-the-value"><i class="fas fa-hashtag"></i></a></h2></h2><p>At this point, here’s our full <code class="language-plaintext highlighter-rouge">Value</code> enum:</p><div class="language-rust highlighter-rouge"><div class="code-header"> <span label-text="Rust"><i class="fas fa-code small"></i></span> <button aria-label="copy" title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
</pre><td class="rouge-code"><pre><span class="k">pub</span> <span class="k">struct</span> <span class="n">Closure</span> <span class="p">{</span>
    <span class="k">pub</span> <span class="n">callable_index</span><span class="p">:</span> <span class="nb">usize</span>
<span class="p">}</span>

<span class="k">pub</span> <span class="k">enum</span> <span class="n">Value</span> <span class="p">{</span>
    <span class="nf">Int</span><span class="p">(</span><span class="nb">i32</span><span class="p">),</span>
    <span class="nf">Bool</span><span class="p">(</span><span class="nb">bool</span><span class="p">),</span>
    <span class="nf">Str</span><span class="p">(</span><span class="o">&amp;</span><span class="k">'static</span> <span class="nb">str</span><span class="p">),</span>
    <span class="nf">IntTuple</span><span class="p">(</span><span class="nb">i32</span><span class="p">,</span> <span class="nb">i32</span><span class="p">),</span>
    <span class="nf">BoolIntTuple</span><span class="p">(</span><span class="nb">bool</span><span class="p">,</span> <span class="nb">i32</span><span class="p">),</span>
    <span class="nf">IntBoolTuple</span><span class="p">(</span><span class="nb">i32</span><span class="p">,</span> <span class="nb">bool</span><span class="p">),</span>
    <span class="nf">BoolTuple</span><span class="p">(</span><span class="nb">bool</span><span class="p">,</span> <span class="nb">bool</span><span class="p">),</span>
    <span class="nf">DynamicTuple</span><span class="p">(</span><span class="nb">Box</span><span class="o">&lt;</span><span class="n">Value</span><span class="o">&gt;</span><span class="p">,</span> <span class="nb">Box</span><span class="o">&lt;</span><span class="n">Value</span><span class="o">&gt;</span><span class="p">),</span>
    <span class="nf">Closure</span><span class="p">(</span><span class="n">Closure</span><span class="p">),</span>
    <span class="nf">Trampoline</span><span class="p">(</span><span class="n">Closure</span><span class="p">),</span>
    <span class="nf">Error</span><span class="p">(</span><span class="o">&amp;</span><span class="k">'static</span> <span class="nb">str</span><span class="p">),</span>
<span class="p">}</span>
</pre></table></code></div></div><p>There’s a problem: The value is a whopping 24 bytes in size. That’s bad. On a 64-bit computer, we can handle 8 bytes at a time, and we’re 3x larger than that. This enum is 24 bytes because it uses 1 byte for the tag, but the values inside the enum variant need to be aligned, so Rust determined an extra 7 bytes of padding is needed. Then we need enough space for the largest variant, which is <code class="language-plaintext highlighter-rouge">DynamicTuple(Box&lt;Value&gt;, Box&lt;Value&gt;)</code>, used when tuples contain values other than Int or Bool. Each Box needs 8 bytes, so there are 16 bytes for the value, plus 8 bytes for the aligned tag.</p><p>At the time I wrote the optimizations we’ll see, I did not know about NaN tagging. NaN tagging is a technique where you represent every value as a NaN variant. Turns out there are many many IEE754 double values that are NaN, and we can use silent NaNs and manipulate their bits to indicate what kind of value they are.</p><p>The benefit of NaN tagging comes when our language has support for double values. Whenever we need a double value, we can just use the value as-is. Rinha, however, has no doubles, only 32-bit signed integers, so the usefulness of NaN tagging is limited. However, NaN tagging also handles pointers: In most machines, pointers only really use 48 bits of information. Maybe in the future, I will implement something similar to NaN tagging. This technique should alleviate the impact of padding space that Rust adds to enums to ensure alignment, though it might create other overheads.</p><p>Therefore, what I did was an emulation of 32-bit pointers in the most hacky way possible: Just indices on a Vec. My heap is really just a Vec. I store values in a <code class="language-plaintext highlighter-rouge">Vec</code> and use a <code class="language-plaintext highlighter-rouge">u32</code> index instead of <code class="language-plaintext highlighter-rouge">usize</code>. You can allocate at most ~4 billion tuples, ~4 billion strings, and ~4 billion closures in my interpreter… it’s a price we pay for performance. This is the new definition:</p><div class="language-rust highlighter-rouge"><div class="code-header"> <span label-text="Rust"><i class="fas fa-code small"></i></span> <button aria-label="copy" title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
</pre><td class="rouge-code"><pre><span class="k">pub</span> <span class="k">struct</span> <span class="n">Closure</span> <span class="p">{</span>
    <span class="k">pub</span> <span class="n">callable_index</span><span class="p">:</span> <span class="nb">u32</span><span class="p">,</span>
    <span class="k">pub</span> <span class="n">closure_env_index</span><span class="p">:</span> <span class="nb">u32</span>
<span class="p">}</span>
<span class="k">pub</span> <span class="k">struct</span> <span class="nf">HeapPointer</span><span class="p">(</span><span class="nb">u32</span><span class="p">);</span>
<span class="k">pub</span> <span class="k">struct</span> <span class="nf">StringPointer</span><span class="p">(</span><span class="nb">u32</span><span class="p">);</span>

<span class="nd">#[derive(Clone,</span> <span class="nd">Debug,</span> <span class="nd">Eq,</span> <span class="nd">PartialEq,</span> <span class="nd">PartialOrd,</span> <span class="nd">Ord)]</span>
<span class="k">pub</span> <span class="k">enum</span> <span class="n">Value</span> <span class="p">{</span>
    <span class="nf">Int</span><span class="p">(</span><span class="nb">i32</span><span class="p">),</span>
    <span class="nf">Bool</span><span class="p">(</span><span class="nb">bool</span><span class="p">),</span>
    <span class="nf">Str</span><span class="p">(</span><span class="n">StringPointer</span><span class="p">),</span>
    <span class="nf">IntTuple</span><span class="p">(</span><span class="nb">i32</span><span class="p">,</span> <span class="nb">i32</span><span class="p">),</span>
    <span class="nf">BoolIntTuple</span><span class="p">(</span><span class="nb">bool</span><span class="p">,</span> <span class="nb">i32</span><span class="p">),</span>
    <span class="nf">IntBoolTuple</span><span class="p">(</span><span class="nb">i32</span><span class="p">,</span> <span class="nb">bool</span><span class="p">),</span>
    <span class="nf">BoolTuple</span><span class="p">(</span><span class="nb">bool</span><span class="p">,</span> <span class="nb">bool</span><span class="p">),</span>
    <span class="nf">DynamicTuple</span><span class="p">(</span><span class="n">HeapPointer</span><span class="p">,</span> <span class="n">HeapPointer</span><span class="p">),</span>
    <span class="nf">Closure</span><span class="p">(</span><span class="n">Closure</span><span class="p">),</span>
    <span class="nf">Trampoline</span><span class="p">(</span><span class="n">Closure</span><span class="p">),</span>
<span class="p">}</span>
</pre></table></code></div></div><p>This is 12 bytes. Now Rust chose a 4-byte alignment due to HeapPointer and values inside Closure being 4 bytes. Notice I also got rid of the Error variant, it was useless. Now we can fit many more Values in the CPU cache and they are much faster to copy. Now we are down to 2.9ms from 3.4ms. 17% faster than the previous, and 1386% from baseline.</p><h2 id="optimization-8-reduce-the-size-of-the-value-even-more">Optimization 8: Reduce the size of the <code class="language-plaintext highlighter-rouge">Value</code> even more<a href="#optimization-8-reduce-the-size-of-the-value-even-more"><i class="fas fa-hashtag"></i></a></h2></h2><p>This is just an extension of the previous idea. Let’s get it down to 8 bytes by creating a <code class="language-plaintext highlighter-rouge">ClosurePointer</code> of 4 bytes. I also had to remove the extra tuple values and make all of them point to heap values, otherwise, the size of the enum would still be 12 bytes because i32 and bool align in 4 bytes.</p><p>We’re at 2.5ms, 16% improvement over last, 1624% over baseline.</p><h2 id="optimization-9-frame-reuse-in-tco">Optimization 9: Frame reuse in TCO<a href="#optimization-9-frame-reuse-in-tco"><i class="fas fa-hashtag"></i></a></h2></h2><p>Currently, our TCO implementation just avoids creating a physical call frame on every call inside the recursive function, but it still creates a virtual VM frame. This creates a lot of unnecessary allocations.</p><p>In the end, I spent a whole afternoon making it work, but the implementation ended up being really simple: a flag on the call frame that tells whether we should reuse the fame. Then calling the function recursively, we read that flag and simply skip popping and pushing that frame. Only when the trampoliner finishes executing we really pop the frame.</p><p>This reduced the runtime to 2.2ms, 13.6% faster than the previous, and 1859% over baseline.</p><h2 id="optimization-10-empty-closures">Optimization 10: Empty closures<a href="#optimization-10-empty-closures"><i class="fas fa-hashtag"></i></a></h2></h2><p>Some functions don’t capture values outside of them. However, the interpreter still allocates a closure value for every function, including ones that are frequently called but capture nothing.</p><p>When we compile a function, we store it in a vector and pass a slice of this vector to the execution context:</p><div class="language-rust highlighter-rouge"><div class="code-header"> <span label-text="Rust"><i class="fas fa-code small"></i></span> <button aria-label="copy" title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
</pre><td class="rouge-code"><pre><span class="k">pub</span> <span class="k">struct</span> <span class="n">Closure</span> <span class="p">{</span>
    <span class="k">pub</span> <span class="n">callable_index</span><span class="p">:</span> <span class="nb">usize</span><span class="p">,</span>
    <span class="k">pub</span> <span class="n">closure_env_index</span><span class="p">:</span> <span class="nb">usize</span>
<span class="p">}</span>

<span class="k">pub</span> <span class="k">struct</span> <span class="n">Callable</span> <span class="p">{</span>
    <span class="k">pub</span> <span class="n">parameters</span><span class="p">:</span> <span class="o">&amp;</span><span class="k">'static</span> <span class="p">[</span><span class="nb">usize</span><span class="p">],</span>
    <span class="k">pub</span> <span class="n">closure_indices</span><span class="p">:</span> <span class="o">&amp;</span><span class="k">'static</span> <span class="p">[</span><span class="nb">usize</span><span class="p">],</span>
    <span class="k">pub</span> <span class="n">body</span><span class="p">:</span> <span class="n">LambdaFunction</span><span class="p">,</span>
    <span class="k">pub</span> <span class="n">trampoline_of</span><span class="p">:</span> <span class="nb">Option</span><span class="o">&lt;</span><span class="nb">usize</span><span class="o">&gt;</span>
<span class="p">}</span>


<span class="k">pub</span> <span class="k">struct</span> <span class="n">ExecutionContext</span><span class="o">&lt;</span><span class="nv">'a</span><span class="o">&gt;</span> <span class="p">{</span>
    <span class="o">...</span>
    <span class="k">pub</span> <span class="n">functions</span><span class="p">:</span> <span class="o">&amp;</span><span class="nv">'a</span> <span class="p">[</span><span class="n">Callable</span><span class="p">],</span>
    <span class="k">pub</span> <span class="n">closures</span><span class="p">:</span> <span class="nb">Vec</span><span class="o">&lt;</span><span class="n">Closure</span><span class="o">&gt;</span><span class="p">,</span>
    <span class="k">pub</span> <span class="n">closure_environments</span><span class="p">:</span> <span class="nb">Vec</span><span class="o">&lt;</span><span class="n">BTreeMap</span><span class="o">&lt;</span><span class="nb">usize</span><span class="p">,</span> <span class="n">Value</span><span class="o">&gt;&gt;</span><span class="p">,</span>
    <span class="o">...</span>
<span class="p">}</span>
</pre></table></code></div></div><p>For closures, at this point, we are using <code class="language-plaintext highlighter-rouge">BTreeMaps</code>. Somewhere down the line I actually started using <code class="language-plaintext highlighter-rouge">Vec</code>, but I forgot to document this change. The usize key is the variable ID. Every time we declare a function, we create a new <code class="language-plaintext highlighter-rouge">BTreeMap</code> and store it in the vector, which makes the <code class="language-plaintext highlighter-rouge">closure_environments</code> grow rapidly. But there are many cases where this is not necessary: If the compiler analyzes the function and decides it doesn’t capture anything, we can just use one canonical “capture-less” function.</p><p>Therefore, at the beginning of the execution during instantiation of the <code class="language-plaintext highlighter-rouge">ExecutionContext</code>, for every callable we store a Closure that points to a non-existent <code class="language-plaintext highlighter-rouge">closure_env_index</code>, like <code class="language-plaintext highlighter-rouge">u32::MAX</code>. This points to a value that doesn’t exist and would crash if used, but the fact that it tried to load a non-existing closure is a bug in our analysis step.</p><div class="language-rust highlighter-rouge"><div class="code-header"> <span label-text="Rust"><i class="fas fa-code small"></i></span> <button aria-label="copy" title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
</pre><td class="rouge-code"><pre>  <span class="k">let</span> <span class="n">empty_closures</span> <span class="o">=</span> <span class="p">{</span>
      <span class="k">let</span> <span class="k">mut</span> <span class="n">closures</span> <span class="o">=</span> <span class="nd">vec!</span><span class="p">[];</span>
      <span class="k">for</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">_</span><span class="p">)</span> <span class="k">in</span> <span class="n">program</span><span class="py">.functions</span><span class="nf">.iter</span><span class="p">()</span><span class="nf">.enumerate</span><span class="p">()</span> <span class="p">{</span>
          <span class="n">closures</span><span class="nf">.push</span><span class="p">(</span><span class="n">Closure</span> <span class="p">{</span>
              <span class="n">callable_index</span><span class="p">:</span> <span class="n">i</span><span class="p">,</span>
              <span class="n">closure_env_index</span><span class="p">:</span> <span class="nn">usize</span><span class="p">::</span><span class="n">MAX</span><span class="p">,</span>
          <span class="p">})</span>
      <span class="p">}</span>
      <span class="n">closures</span>
  <span class="p">};</span>
</pre></table></code></div></div><p>Whenever we find a non-capturing function, we evaluate it as a <code class="language-plaintext highlighter-rouge">Closure { callable_index, closure_env_index: u32::MAX }</code> and return a <code class="language-plaintext highlighter-rouge">ClosurePointer(callable_index)</code>. As you can see in the code above, the callable index points to the actual callable index in the functions vec.</p><p>This entire thing improved the performance by….. 0%. Nice :) Some benchmarks did run without exploding memory though, so that’s good.</p><h3 id="optimization-11-small-tuples">Optimization 11: Small Tuples<a href="#optimization-11-small-tuples"><i class="fas fa-hashtag"></i></a></h3></h3><p>For tuples with int values that fit into i16, we don’t heap allocate them, instead, I created a <code class="language-plaintext highlighter-rouge">SmallTuple(i16, i16)</code> variant. Down to 2.1ms.</p><h3 id="the-bug-fixes">The bug fixes<a href="#the-bug-fixes"><i class="fas fa-hashtag"></i></a></h3></h3><p>So many optimizations done, everything must be running just fine, right? Well… it turns out I had some massive bugs regarding closures. Sometimes values weren’t being loaded from where they should be. Basically, something like this would happen:</p><div class="language-js highlighter-rouge"><div class="code-header"> <span label-text="JavaScript"><i class="fas fa-code small"></i></span> <button aria-label="copy" title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre><td class="rouge-code"><pre><span class="kd">let</span> <span class="nx">y</span> <span class="o">=</span> <span class="mi">10</span><span class="p">;</span>
<span class="kd">let</span> <span class="nx">f</span> <span class="o">=</span> <span class="nf">fn</span><span class="p">()</span> <span class="p">{</span> <span class="nx">y</span> <span class="p">};</span>
<span class="kd">let</span> <span class="nx">y</span> <span class="o">=</span> <span class="mi">20</span><span class="p">;</span> <span class="c1">//shadowed!</span>
<span class="nf">print</span><span class="p">(</span><span class="nx">y</span><span class="p">);</span> <span class="c1">//should print 10, not 20</span>
</pre></table></code></div></div><p>To my horror, it was printing 20 because values weren’t being copied into the closure properly. Unfortunately, after fixing it, the performance got much worse, up to 3.4ms. That’s a huge setback.</p><p>However, the fix was quite easy.</p><h3 id="optimization-12-minimal-function-closures">Optimization 12: Minimal function closures<a href="#optimization-12-minimal-function-closures"><i class="fas fa-hashtag"></i></a></h3></h3><p>Our capture analysis in Optimization 10 missed the opportunity to reduce the copies. The way closures were created was:</p><ul><li>Get all the variables in scope right now<li>Copy them into the closure<li>GG</ul><p>However, the closures usually need only a subset of these variables. Turns out I had that information in hand, so it was easy to use that. Down to 2.2ms, almost pre-bugfixing levels.</p><p>This was the program I submitted for the competition.</p><h3 id="one-more-hiccup">One more hiccup…<a href="#one-more-hiccup"><i class="fas fa-hashtag"></i></a></h3></h3><p>This one got almost everyone by surprise. Sofia tested her benchmark program only to break almost all interpreters, and not for the reasons we expected.</p><p>This is the code that they tried to run, which isn’t that big, having almost 400 lines of code. You can take a look at it <a href="https://gist.github.com/aripiprazole/d46c315d6923c64ad5082b6d221e83e8">here</a>. Each <code class="language-plaintext highlighter-rouge">let</code> statement has a <code class="language-plaintext highlighter-rouge">next</code> node, which could point to another <code class="language-plaintext highlighter-rouge">let</code>, that has another <code class="language-plaintext highlighter-rouge">next</code>, and so on. In my case, Serde would just not parse that file. It complained about “maximum recursion depth” or something, so I had to change some configs and add some dependencies, and then I got afraid my compiler would crash due to excessive recursion. I thought they had thousands and thousands of lines of code.</p><p>After changing some stuff (using lists of let expressions instead of a linked list of <code class="language-plaintext highlighter-rouge">let</code>), I fixed the issue but lost a bit of performance, now the program runs in 2.3ms. I would only understand why a few days later, but at this time, I had to quickly submit a fix.</p><p>At this point, my interpreter was running fib(46) in around 220 seconds, while the winning submission from Raphael was running it in 140 seconds without memoization. Ah yes, some people implemented memoization, to varying levels of success. Raphael did too, but this 140s number was acquired when his interpreter didn’t have memoization.</p><p>The next optimizations were done after the competition.</p><h3 id="optimization-13-undo-optimization-3">Optimization 13: Undo optimization 3<a href="#optimization-13-undo-optimization-3"><i class="fas fa-hashtag"></i></a></h3></h3><p>In Optimization 3, we did that weird variable stack tracking that resembled, IMO, a column-based database. I decided to get rid of it, and similar to Optimization 12, compute exactly how much space the stack frame needs. I had the information available. Then I changed the Call frame to store a <code class="language-plaintext highlighter-rouge">Vec&lt;Value&gt;</code> inside it. We got down to 2.0ms from 2.3ms, which is the fastest so far. But this revealed some subtle bugs that I had to fix, so it’s also the fastest and most correct implementation at this point.</p><p>The next 2 implementations were very easy and had massive effects.</p><h3 id="optimization-14-solving-the-pre-submission-slowdown">Optimization 14: Solving the pre-submission slowdown.<a href="#optimization-14-solving-the-pre-submission-slowdown"><i class="fas fa-hashtag"></i></a></h3></h3><p>What happened in the pre-submission bugfix?</p><p>Well, <code class="language-plaintext highlighter-rouge">let</code> statements became vectors instead of linked lists from the AST format. Because of that, bodies of functions and if statements had lists of expressions that had to be run like this:</p><div class="language-rust highlighter-rouge"><div class="code-header"> <span label-text="Rust"><i class="fas fa-code small"></i></span> <button aria-label="copy" title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
</pre><td class="rouge-code"><pre><span class="k">let</span> <span class="n">all_statements</span> <span class="o">=</span> <span class="k">self</span><span class="nf">.compile_exprs</span><span class="p">(</span><span class="n">body</span><span class="p">);</span>
<span class="nn">Box</span><span class="p">::</span><span class="nf">new</span><span class="p">(</span><span class="k">move</span> <span class="p">|</span><span class="n">ec</span><span class="p">,</span> <span class="n">frame</span><span class="p">|</span> <span class="p">{</span>
    <span class="k">let</span> <span class="k">mut</span> <span class="n">last</span> <span class="o">=</span> <span class="nb">None</span><span class="p">;</span>
    <span class="k">for</span> <span class="n">l</span> <span class="k">in</span> <span class="n">leaked</span> <span class="p">{</span>
        <span class="n">last</span> <span class="o">=</span> <span class="nf">Some</span><span class="p">(</span><span class="nf">l</span><span class="p">(</span><span class="n">ec</span><span class="p">,</span> <span class="n">frame</span><span class="p">));</span>
    <span class="p">}</span>
    <span class="n">last</span><span class="nf">.unwrap</span><span class="p">()</span>
<span class="p">});</span>
</pre></table></code></div></div><p>It turns out this whole looping machinery causes a lot of overhead when we only have one simple expression that, in the end, just returns a variable or a literal value. If only we knew how many statements we had in the body, we could do something about it…</p><div class="language-rust highlighter-rouge"><div class="code-header"> <span label-text="Rust"><i class="fas fa-code small"></i></span> <button aria-label="copy" title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
</pre><td class="rouge-code"><pre><span class="k">fn</span> <span class="nf">join_lambdas</span><span class="p">(</span><span class="o">&amp;</span><span class="k">mut</span> <span class="k">self</span><span class="p">,</span> <span class="k">mut</span> <span class="n">lambdas</span><span class="p">:</span> <span class="nb">Vec</span><span class="o">&lt;</span><span class="n">LambdaFunction</span><span class="o">&gt;</span><span class="p">)</span> <span class="k">-&gt;</span> <span class="n">LambdaFunction</span> <span class="p">{</span>
  <span class="k">if</span> <span class="n">lambdas</span><span class="nf">.len</span><span class="p">()</span> <span class="o">==</span> <span class="mi">1</span> <span class="p">{</span>
      <span class="k">return</span> <span class="n">lambdas</span><span class="nf">.pop</span><span class="p">()</span><span class="nf">.unwrap</span><span class="p">();</span>
  <span class="p">}</span>

  <span class="nn">Box</span><span class="p">::</span><span class="nf">new</span><span class="p">(</span><span class="k">move</span> <span class="p">|</span><span class="n">ec</span><span class="p">|</span> <span class="p">{</span>
    <span class="k">let</span> <span class="k">mut</span> <span class="n">last</span> <span class="o">=</span> <span class="nb">None</span><span class="p">;</span>
    <span class="k">for</span> <span class="n">l</span> <span class="k">in</span> <span class="n">leaked</span> <span class="p">{</span>
        <span class="n">last</span> <span class="o">=</span> <span class="nf">Some</span><span class="p">(</span><span class="nf">l</span><span class="p">(</span><span class="n">ec</span><span class="p">));</span>
    <span class="p">}</span>
    <span class="n">last</span><span class="nf">.unwrap</span><span class="p">()</span>
  <span class="p">});</span>
<span class="p">}</span>
</pre></table></code></div></div><p>With this change, <code class="language-plaintext highlighter-rouge">perf.rinha</code> barely changed, from 2.0ms to 1.9ms. However, fib(30) went from 110ms down to 77ms(!). This is a massive gain.</p><p>Other loop-heavy code also got significantly faster. For instance, this code that counts to 2 billion:</p><div class="language-js highlighter-rouge"><div class="code-header"> <span label-text="JavaScript"><i class="fas fa-code small"></i></span> <button aria-label="copy" title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
</pre><td class="rouge-code"><pre><span class="kd">let</span> <span class="nx">loop</span> <span class="o">=</span> <span class="nf">fn </span><span class="p">(</span><span class="nx">i</span><span class="p">,</span> <span class="nx">s</span><span class="p">)</span> <span class="o">=&gt;</span> <span class="p">{</span>
   <span class="k">if </span><span class="p">(</span><span class="nx">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
     <span class="nx">s</span>
   <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
      <span class="nf">loop</span><span class="p">(</span><span class="nx">i</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="nx">s</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
   <span class="p">}</span>
<span class="p">};</span>
<span class="nf">print</span><span class="p">(</span><span class="nf">loop</span><span class="p">(</span><span class="mi">2000000000</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
</pre></table></code></div></div><p>This used to run in 60 seconds and now runs in 47 seconds, rivaling the fastest interpreter among the people in the Discord chat (the 7th place submission from fabiosvm).</p><h3 id="final-optimization-pass-the-call-frame-along">Final optimization: Pass the call frame along<a href="#final-optimization-pass-the-call-frame-along"><i class="fas fa-hashtag"></i></a></h3></h3><p>Whenever we build a lambda function, we do this:</p><div class="language-rust highlighter-rouge"><div class="code-header"> <span label-text="Rust"><i class="fas fa-code small"></i></span> <button aria-label="copy" title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre>  <span class="nn">Box</span><span class="p">::</span><span class="nf">new</span><span class="p">(</span><span class="k">move</span> <span class="p">|</span><span class="n">ec</span><span class="p">|</span> <span class="p">{</span> <span class="n">ec</span><span class="nf">.frame</span><span class="p">()</span><span class="py">.stack_data</span><span class="o">...</span> <span class="cm">/*do something with frame data*/</span> <span class="p">})</span>
</pre></table></code></div></div><p>The <code class="language-plaintext highlighter-rouge">ec.frame()</code> call returns the top of the call stack, which is stored in a <code class="language-plaintext highlighter-rouge">Vec</code> inside the execution context. However, why do this when the call stack is quite explicit in our program? Why save in a vector, which needs to do a size check for every push even if enough memory exists?</p><p>Turns out we can just do this:</p><div class="language-rust highlighter-rouge"><div class="code-header"> <span label-text="Rust"><i class="fas fa-code small"></i></span> <button aria-label="copy" title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre><td class="rouge-code"><pre><span class="nn">Box</span><span class="p">::</span><span class="nf">new</span><span class="p">(</span><span class="k">move</span> <span class="p">|</span><span class="n">ec</span><span class="p">,</span> <span class="n">frame</span><span class="p">|</span> <span class="p">{</span> <span class="n">frame</span><span class="py">.stack_data</span><span class="o">...</span> <span class="cm">/*do something with frame data*/</span> <span class="p">})</span>
                   <span class="n">ˆˆˆˆˆ</span> <span class="n">new</span> <span class="n">parameter</span>
</pre></table></code></div></div><p>Just pass the frames along. When we call a function, we create the new frame and pass it to the callee’s compiled <code class="language-plaintext highlighter-rouge">LambdaFunction</code>.</p><p>This results in a massive improvement.</p><p><code class="language-plaintext highlighter-rouge">perf.rinha</code> now runs in 1.55ms, down from 1.95ms. This is where I achieved the 27x speedup. <code class="language-plaintext highlighter-rouge">fib(46)</code> runs in 120s from 190ms of the last optimization and the big looping function that counts to a billion now runs in about 35 seconds instead of a full minute.</p><p>I think if we ran the benchmark again I would have some chance of getting a better position ;)</p><h1 id="conclusion">Conclusion</h1><p>I had a blast working on Rinha. At some points, I had to fire up callgrind and cachegrind to see what was going on, had to do a bunch of experiments that ended up not working, but it was fun experimenting with different techniques. Even if I didn’t win the competition, I had a lot of fun programming and talking with people on Discord and exchanging ideas.</p><p>Maybe the next challenge is to fully type-check the language and use LLVM, who knows?</p><p>Or maybe I should just continue working on my other compiler project, Donkey, which is a statically typed (with type annotations) Python-like programming language that tastes like C with some C++-template-inspired generics. Or maybe rest a little bit :)</p></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/rust/'>Rust</a>, <a href='/categories/interpreters/'>Interpreters</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/rust/" class="post-tag no-text-decoration" >rust</a> <a href="/tags/interpreters/" class="post-tag no-text-decoration" >interpreters</a> <a href="/tags/tree-walker/" class="post-tag no-text-decoration" >tree walker</a> <a href="/tags/bytecode/" class="post-tag no-text-decoration" >bytecode</a> <a href="/tags/virtual-machine/" class="post-tag no-text-decoration" >virtual machine</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=How I improved my interpreter speed by 27x - A Compiler/Interpreter Competition - Ricardo Pieper&url=https://ricardopieper.github.io//posts/rinha-compiler/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=How I improved my interpreter speed by 27x - A Compiler/Interpreter Competition - Ricardo Pieper&u=https://ricardopieper.github.io//posts/rinha-compiler/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://telegram.me/share?text=How I improved my interpreter speed by 27x - A Compiler/Interpreter Competition - Ricardo Pieper&url=https://ricardopieper.github.io//posts/rinha-compiler/" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" title-succeed="Link copied successfully!"> </i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down"><div class="access"><div id="access-lastmod" class="post"><div class="panel-heading">Recent Update</div><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/kubernetes-gke-grcp-aspnet/">Deploying an ASP.NET gRPC service on GKE with GCE Ingress</a><li><a href="/posts/redis-pitfalls/">The pitfalls of Redis</a><li><a href="/posts/rinha-compiler/">How I improved my interpreter speed by 27x - A Compiler/Interpreter Competition</a></ul></div><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/bytecode/">bytecode</a> <a class="post-tag" href="/tags/concurrent/">concurrent</a> <a class="post-tag" href="/tags/distributed/">distributed</a> <a class="post-tag" href="/tags/interpreters/">interpreters</a> <a class="post-tag" href="/tags/kubernetes-gke-gce/">kubernetes gke gce</a> <a class="post-tag" href="/tags/redis/">redis</a> <a class="post-tag" href="/tags/rust/">rust</a> <a class="post-tag" href="/tags/tree-walker/">tree walker</a> <a class="post-tag" href="/tags/virtual-machine/">virtual machine</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"><div class="panel-heading pl-3 pt-2 mb-2">Contents</div><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="tail-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/redis-pitfalls/"><div class="card-body"> <em class="timeago small" date="2023-11-07 12:00:00 -0300" >Nov 7</em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>The pitfalls of Redis</h3><div class="text-muted small"><p> Over many years, I’ve been using Redis for caching and as a “distributed data structure server”. I love it. As an extremely lightweight in-memory database that Just Works™️, I’ve seen developers sl...</p></div></div></a></div><div class="card"> <a href="/posts/kubernetes-gke-grcp-aspnet/"><div class="card-body"> <em class="timeago small" date="2021-12-11 03:00:00 -0300" >Dec 11, 2021</em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Deploying an ASP.NET gRPC service on GKE with GCE Ingress</h3><div class="text-muted small"><p> Context At Monomyto Game Studios, we use Kubernetes to manage our infrastructure. Our backend team consists, currently, of just one person (me), so I’d like to keep things as simple as possible, w...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/kubernetes-gke-grcp-aspnet/" class="btn btn-outline-primary" prompt="Older"><p>Deploying an ASP.NET gRPC service on GKE with GCE Ingress</p></a> <a href="/posts/redis-pitfalls/" class="btn btn-outline-primary" prompt="Newer"><p>The pitfalls of Redis</p></a></div></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center text-muted"><div class="footer-left"><p class="mb-0"> © 2023 <a href="https://twitter.com/ricardopieper1">Ricardo Pieper</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/bytecode/">bytecode</a> <a class="post-tag" href="/tags/concurrent/">concurrent</a> <a class="post-tag" href="/tags/distributed/">distributed</a> <a class="post-tag" href="/tags/interpreters/">interpreters</a> <a class="post-tag" href="/tags/kubernetes-gke-gce/">kubernetes gke gce</a> <a class="post-tag" href="/tags/redis/">redis</a> <a class="post-tag" href="/tags/rust/">rust</a> <a class="post-tag" href="/tags/tree-walker/">tree walker</a> <a class="post-tag" href="/tags/virtual-machine/">virtual machine</a></div></div></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/lozad/dist/lozad.min.js,npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.16.1,npm/bootstrap@4/dist/js/bootstrap.min.js"></script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id="></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', ''); }); </script>
